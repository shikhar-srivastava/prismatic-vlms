{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been written to results_CL.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the input and output file names\n",
    "input_file = 'results_nlp.json'\n",
    "output_file = 'results_CL.json'\n",
    "\n",
    "# Load the JSON data from the input file\n",
    "with open(input_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter out keys that start with 'cl-'\n",
    "filtered_data = {key: value for key, value in data.items() if key.startswith('cl-') or key == 'reproduction-align-pythia+410m'}\n",
    "\n",
    "# Write the filtered data to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)\n",
    "\n",
    "print(f\"Filtered data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 'cl-instruct-stage-0-pythia+410m-lora' missing for model 'lora'\n",
      "Run 'cl-vqa-stage-1-pythia+410m-sgm-rehearsal1' missing for model 'sgm-rehearsal1'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 160\u001b[0m\n\u001b[1;32m    147\u001b[0m     task_forget \u001b[38;5;241m=\u001b[39m cl_performance_change[model_name]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_avg\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVL\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLU\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLG\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan})\n\u001b[1;32m    149\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    150\u001b[0m         name_mapping[model_name],\n\u001b[1;32m    151\u001b[0m         task_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, task_forget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    152\u001b[0m         task_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLU\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, task_forget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLU\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    153\u001b[0m         task_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, task_forget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     latex_code \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{model}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t1_vl_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t1_vl_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t2_vl_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t2_vl_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t3_vl_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t3_vl_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t4_vl_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t4_vl_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t1_nlu_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t1_nlu_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t2_nlu_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t2_nlu_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t3_nlu_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t3_nlu_d:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t4_nlu_a:.1f}\u001b[39;00m\u001b[38;5;124m & \u001b[39m\u001b[38;5;132;01m{t4_nlu_d:.1f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    156\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    157\u001b[0m         t1_vl_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m1\u001b[39m], t1_vl_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    158\u001b[0m         t2_vl_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m3\u001b[39m], t2_vl_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m    159\u001b[0m         t3_vl_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m5\u001b[39m], t3_vl_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m6\u001b[39m],\n\u001b[0;32m--> 160\u001b[0m         t4_vl_a\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m, t4_vl_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m    161\u001b[0m         t1_nlu_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m9\u001b[39m], t1_nlu_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m    162\u001b[0m         t2_nlu_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m11\u001b[39m], t2_nlu_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m12\u001b[39m],\n\u001b[1;32m    163\u001b[0m         t3_nlu_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m13\u001b[39m], t3_nlu_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m14\u001b[39m],\n\u001b[1;32m    164\u001b[0m         t4_nlu_a\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m15\u001b[39m], t4_nlu_d\u001b[38;5;241m=\u001b[39mmodel_results[\u001b[38;5;241m16\u001b[39m]\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    167\u001b[0m latex_code \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmidrule\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Include the rest of the models\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset stages and the corresponding labels\n",
    "stages = [\"Instruct (0)\", \"VQA (1)\", \"OCR (2)\", \"Ref (3)\"]\n",
    "datasets = [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\"]\n",
    "nlu_nlg_datasets = [\"wsc273\", \"winogrande\", \"lambada_standard\", \"arc_easy\", \"arc_challenge\"]\n",
    "vl_datasets = datasets\n",
    "\n",
    "# Define the mitigation methods and their sequence of model names\n",
    "cl_runs = {\n",
    "    \"naive-ft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m\",\n",
    "        \"cl-ocr-stage-2-pythia+410m\",\n",
    "        \"cl-ref-stage-3-pythia+410m\"\n",
    "    ],\n",
    "    \"sgm\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm\"\n",
    "    ],\n",
    "    \"sgm-olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-olf\"\n",
    "    ],\n",
    "    \"soft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-soft\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-soft\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-soft\",\n",
    "        \"cl-ref-stage-3-pythia+410m-soft\"\n",
    "    ],\n",
    "    \"ia3\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-ia3\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-ia3\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-ia3\",\n",
    "        \"cl-ref-stage-3-pythia+410m-ia3\"\n",
    "    ],\n",
    "    \"lora\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-lora\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-lora\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-lora\",\n",
    "        \"cl-ref-stage-3-pythia+410m-lora\"\n",
    "    ],\n",
    "    \"rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-rehearsal1\"\n",
    "    ],\n",
    "    \"sgm-rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-rehearsal1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Baseline run_id\n",
    "baseline_run_id = \"reproduction-align-pythia+410m\"\n",
    "\n",
    "# Load the JSON data from the results file\n",
    "with open('results_CL.json', 'r') as file:\n",
    "    result = json.load(file)\n",
    "\n",
    "# Check for the existence of baseline results\n",
    "baseline_results = result.get(baseline_run_id)\n",
    "if not baseline_results:\n",
    "    raise ValueError(f\"Baseline run ID '{baseline_run_id}' not found in results.\")\n",
    "\n",
    "# Calculate performance changes and averages for each CL run\n",
    "cl_performance_change = {}\n",
    "cl_performance = {}\n",
    "\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    changes = {}\n",
    "    performances = {}\n",
    "    missing_data = False\n",
    "\n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        current_results = result.get(run_id)\n",
    "        if not current_results:\n",
    "            missing_data = True\n",
    "            print(f\"Run '{run_id}' missing for model '{model_name}'\")\n",
    "            break\n",
    "        \n",
    "        change = {dataset: current_results.get(dataset, np.nan) - baseline_results.get(dataset, np.nan) for dataset in baseline_results.keys()}\n",
    "        changes[f'stage_{i}'] = change\n",
    "        performances[f'stage_{i}'] = current_results\n",
    "        \n",
    "        avg_delta_vl = np.nanmean([change[dataset] for dataset in vl_datasets])\n",
    "        avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in vl_datasets])\n",
    "        avg_delta_nlu = np.nanmean([change[dataset] for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_delta_nlg = change[\"lambada_standard\"]\n",
    "        avg_acc_nlg = current_results.get(\"lambada_standard\", np.nan)\n",
    "        \n",
    "        changes[f'stage_{i}_avg'] = {'VL': avg_delta_vl, 'NLU': avg_delta_nlu, 'NLG': avg_delta_nlg}\n",
    "        performances[f'stage_{i}_avg'] = {'VL': avg_acc_vl, 'NLU': avg_acc_nlu, 'NLG': avg_acc_nlg}\n",
    "    \n",
    "    if not missing_data:\n",
    "        cl_performance_change[model_name] = changes\n",
    "        cl_performance[model_name] = performances\n",
    "\n",
    "# Save the performance changes and averages to JSON files\n",
    "with open('cl_performance_change.json', 'w') as f:\n",
    "    json.dump(cl_performance_change, f, indent=2)\n",
    "\n",
    "with open('cl_performance.json', 'w') as f:\n",
    "    json.dump(cl_performance, f, indent=2)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "name_mapping = {\n",
    "    'sgm': 'SGM',\n",
    "    'sgm-rehearsal1': 'SGM Rehearsal',\n",
    "    'sgm-olf': 'SGM OLF',\n",
    "    'rehearsal1': 'Rehearsal1',\n",
    "    'lora': 'LoRA',\n",
    "    'naive-ft': 'Naive FT',\n",
    "    'soft': 'Soft',\n",
    "    'ia3': 'IA3'\n",
    "}\n",
    "\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
    "  \\\\label{tab:vl_nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{6}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\multicolumn{6}{c}{\\\\textbf{NLU/NLG}} \\\\\\\\\n",
    "     & \\\\multicolumn{2}{c|}{\\\\textbf{Instruct (0)}} & \\\\multicolumn{2}{c|}{\\\\textbf{VQA (1)}} & \\\\multicolumn{2}{c|}{\\\\textbf{OCR (2)}} & \\\\multicolumn{2}{c|}{\\\\textbf{Ref (3)}} & \\\\multicolumn{2}{c|}{\\\\textbf{Instruct (0)}} & \\\\multicolumn{2}{c|}{\\\\textbf{VQA (1)}} & \\\\multicolumn{2}{c}{\\\\textbf{OCR (2)}} \\\\\\\\\n",
    "     & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Include the Naive-FT benchmark first\n",
    "model_name = 'naive-ft'\n",
    "tasks = cl_performance[model_name]\n",
    "\n",
    "for stage_idx in range(4):\n",
    "    task_perf = tasks.get(f'stage_{stage_idx}_avg', {'VL': np.nan, 'NLU': np.nan, 'NLG': np.nan})\n",
    "    task_forget = cl_performance_change[model_name].get(f'stage_{stage_idx}_avg', {'VL': np.nan, 'NLU': np.nan, 'NLG': np.nan})\n",
    "\n",
    "    model_results = (\n",
    "        name_mapping[model_name],\n",
    "        task_perf['VL'] * 100, task_forget['VL'] * 100,\n",
    "        task_perf['NLU'] * 100, task_forget['NLU'] * 100,\n",
    "        task_perf['NLG'] * 100, task_forget['NLG'] * 100\n",
    "    )\n",
    "    latex_code += \"{model} & {t1_vl_a:.1f} & {t1_vl_d:.1f} & {t2_vl_a:.1f} & {t2_vl_d:.1f} & {t3_vl_a:.1f} & {t3_vl_d:.1f} & {t4_vl_a:.1f} & {t4_vl_d:.1f} & {t1_nlu_a:.1f} & {t1_nlu_d:.1f} & {t2_nlu_a:.1f} & {t2_nlu_d:.1f} & {t3_nlu_a:.1f} & {t3_nlu_d:.1f} & {t4_nlu_a:.1f} & {t4_nlu_d:.1f} \\\\\\\\\\n\".format(\n",
    "        model=model_results[0],\n",
    "        t1_vl_a=model_results[1], t1_vl_d=model_results[2],\n",
    "        t2_vl_a=model_results[3], t2_vl_d=model_results[4],\n",
    "        t3_vl_a=model_results[5], t3_vl_d=model_results[6],\n",
    "        t4_vl_a=model_results[7], t4_vl_d=model_results[8],\n",
    "        t1_nlu_a=model_results[9], t1_nlu_d=model_results[10],\n",
    "        t2_nlu_a=model_results[11], t2_nlu_d=model_results[12],\n",
    "        t3_nlu_a=model_results[13], t3_nlu_d=model_results[14],\n",
    "        t4_nlu_a=model_results[15], t4_nlu_d=model_results[16]\n",
    "    )\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Include the rest of the models\n",
    "for model_name, tasks in cl_performance.items():\n",
    "    if model_name not in name_mapping or model_name == 'naive-ft':\n",
    "        continue\n",
    "\n",
    "    for stage_idx in range(4):\n",
    "        task_perf = tasks.get(f'stage_{stage_idx}_avg', {'VL': np.nan, 'NLU': np.nan, 'NLG': np.nan})\n",
    "        task_forget = cl_performance_change[model_name].get(f'stage_{stage_idx}_avg', {'VL': np.nan, 'NLU': np.nan, 'NLG': np.nan})\n",
    "\n",
    "        model_results = (\n",
    "            name_mapping[model_name],\n",
    "            task_perf['VL'] * 100, task_forget['VL'] * 100,\n",
    "            task_perf['NLU'] * 100, task_forget['NLU'] * 100,\n",
    "            task_perf['NLG'] * 100, task_forget['NLG'] * 100\n",
    "        )\n",
    "        latex_code += \"{model} & {t1_vl_a:.1f} & {t1_vl_d:.1f} & {t2_vl_a:.1f} & {t2_vl_d:.1f} & {t3_vl_a:.1f} & {t3_vl_d:.1f} & {t4_vl_a:.1f} & {t4_vl_d:.1f} & {t1_nlu_a:.1f} & {t1_nlu_d:.1f} & {t2_nlu_a:.1f} & {t2_nlu_d:.1f} & {t3_nlu_a:.1f} & {t3_nlu_d:.1f} & {t4_nlu_a:.1f} & {t4_nlu_d:.1f} \\\\\\\\\\n\".format(\n",
    "            model=model_results[0],\n",
    "            t1_vl_a=model_results[1], t1_vl_d=model_results[2],\n",
    "            t2_vl_a=model_results[3], t2_vl_d=model_results[4],\n",
    "            t3_vl_a=model_results[5], t3_vl_d=model_results[6],\n",
    "            t4_vl_a=model_results[7], t4_vl_d=model_results[8],\n",
    "            t1_nlu_a=model_results[9], t1_nlu_d=model_results[10],\n",
    "            t2_nlu_a=model_results[11], t2_nlu_d=model_results[12],\n",
    "            t3_nlu_a=model_results[13], t3_nlu_d=model_results[14],\n",
    "            t4_nlu_a=model_results[15], t4_nlu_d=model_results[16]\n",
    "        )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
