{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been written to results_CL.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the input and output file names\n",
    "input_file = 'results_nlp.json'\n",
    "output_file = 'results_CL.json'\n",
    "\n",
    "# Load the JSON data from the input file\n",
    "with open(input_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter out keys that start with 'cl-'\n",
    "filtered_data = {key: value for key, value in data.items() if key.startswith('cl-') or key == 'reproduction-align-pythia+410m'}\n",
    "\n",
    "# Write the filtered data to the output file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=4)\n",
    "\n",
    "print(f\"Filtered data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
      "  \\label{tab:vl_nlu_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{8}{c|}{\\textbf{Vision-Language (VL)}} & \\multicolumn{8}{c}{\\textbf{NLU/NLG}} \\\\\n",
      "     & \\multicolumn{2}{c|}{\\textbf{Instruct (0)}} & \\multicolumn{2}{c|}{\\textbf{VQA (1)}} & \\multicolumn{2}{c|}{\\textbf{OCR (2)}} & \\multicolumn{2}{c|}{\\textbf{Ref (3)}} & \\multicolumn{2}{c|}{\\textbf{Instruct (0)}} & \\multicolumn{2}{c|}{\\textbf{VQA (1)}} & \\multicolumn{2}{c|}{\\textbf{OCR (2)}} & \\multicolumn{2}{c}{\\textbf{Ref (3)}} \\\\\n",
      "     & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} & \\textbf{A} & \\textbf{$\\Delta$} \\\\\n",
      "     \\midrule\n",
      "Naive FT & 0.3 & 0.2 & 22.2 & 22.1 & 19.5 & 19.4 & 18.4 & 18.3 & 47.9 & 0.4 & 43.9 & -3.6 & 45.3 & -2.2 & 44.8 & -2.7 \\\\\n",
      "\\midrule\n",
      "SGM & 0.0 & -0.1 & 19.2 & 19.1 & 15.4 & 15.3 & 3.8 & 3.7 & 46.6 & -0.9 & 46.0 & -1.5 & 46.6 & -0.9 & 45.1 & -2.4 \\\\\n",
      "SGM OLF & 0.0 & -0.1 & 17.3 & 17.2 & 15.4 & 15.3 & 13.7 & 13.6 & 47.4 & -0.1 & 46.9 & -0.6 & 47.0 & -0.5 & 44.9 & -2.7 \\\\\n",
      "Soft & 0.6 & 0.5 & 0.2 & 0.1 & 14.9 & 14.8 & 1.7 & 1.6 & 47.0 & -0.5 & 43.7 & -3.8 & 46.2 & -1.3 & 44.3 & -3.3 \\\\\n",
      "IA3 & 0.0 & -0.1 & 12.7 & 12.6 & 12.0 & 11.9 & 12.9 & 12.8 & 48.4 & 0.8 & 48.3 & 0.8 & 48.3 & 0.8 & 48.6 & 1.1 \\\\\n",
      "LoRA & 0.2 & 0.1 & 20.4 & 20.3 & 17.5 & 17.4 & 18.1 & 18.0 & 47.0 & -0.5 & 46.2 & -1.3 & 45.7 & -1.8 & 46.7 & -0.8 \\\\\n",
      "Rehearsal1 & 0.3 & 0.2 & 18.7 & 18.6 & 11.1 & 11.0 & 9.8 & 9.7 & 47.9 & 0.4 & 44.0 & -3.5 & 45.5 & -2.0 & 45.2 & -2.3 \\\\\n",
      "SGM Rehearsal & 0.0 & -0.1 & 18.9 & 18.8 & 17.0 & 16.9 & 18.9 & 18.8 & 46.6 & -0.9 & 47.1 & -0.4 & 46.3 & -1.2 & 45.5 & -2.0 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset stages and the corresponding labels\n",
    "stages = [\"Instruct (0)\", \"VQA (1)\", \"OCR (2)\", \"Ref (3)\"]\n",
    "datasets = [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\"]\n",
    "nlu_nlg_datasets = [\"wsc273\", \"winogrande\", \"lambada_standard\", \"arc_easy\", \"arc_challenge\"]\n",
    "vl_datasets = datasets\n",
    "\n",
    "# Define the mitigation methods and their sequence of model names\n",
    "cl_runs = {\n",
    "    \"naive-ft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m\",\n",
    "        \"cl-ocr-stage-2-pythia+410m\",\n",
    "        \"cl-ref-stage-3-pythia+410m\"\n",
    "    ],\n",
    "    \"sgm\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm\"\n",
    "    ],\n",
    "    \"sgm-olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-olf\"\n",
    "    ],\n",
    "    \"soft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-soft\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-soft\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-soft\",\n",
    "        \"cl-ref-stage-3-pythia+410m-soft\"\n",
    "    ],\n",
    "    \"ia3\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-ia3\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-ia3\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-ia3\",\n",
    "        \"cl-ref-stage-3-pythia+410m-ia3\"\n",
    "    ],\n",
    "    \"lora\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-lora\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-lora\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-lora\",\n",
    "        \"cl-ref-stage-3-pythia+410m-lora\"\n",
    "    ],\n",
    "    \"rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-rehearsal1\"\n",
    "    ],\n",
    "    \"sgm-rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-rehearsal1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Baseline run_id\n",
    "baseline_run_id = \"reproduction-align-pythia+410m\"\n",
    "\n",
    "# Load the JSON data from the results file\n",
    "with open('results_CL.json', 'r') as file:\n",
    "    result = json.load(file)\n",
    "\n",
    "# Check for the existence of baseline results\n",
    "baseline_results = result.get(baseline_run_id)\n",
    "if not baseline_results:\n",
    "    raise ValueError(f\"Baseline run ID '{baseline_run_id}' not found in results.\")\n",
    "\n",
    "# Calculate performance changes and averages for each CL run\n",
    "cl_performance_change = {}\n",
    "cl_performance = {}\n",
    "\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    changes = {}\n",
    "    performances = {}\n",
    "    missing_data = False\n",
    "\n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        current_results = result.get(run_id)\n",
    "        if not current_results:\n",
    "            missing_data = True\n",
    "            print(f\"Run '{run_id}' missing for model '{model_name}'\")\n",
    "            break\n",
    "        \n",
    "        change = {dataset: current_results.get(dataset, np.nan) - baseline_results.get(dataset, np.nan) for dataset in baseline_results.keys()}\n",
    "        changes[f'stage_{i}'] = change\n",
    "        performances[f'stage_{i}'] = current_results\n",
    "        \n",
    "        avg_delta_vl = np.nanmean([change[dataset] for dataset in vl_datasets])\n",
    "        avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in vl_datasets])\n",
    "        avg_delta_nlu = np.nanmean([change[dataset] for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_delta_nlg = change[\"lambada_standard\"]\n",
    "        avg_acc_nlg = current_results.get(\"lambada_standard\", np.nan)\n",
    "        \n",
    "        changes[f'stage_{i}_avg'] = {'VL': avg_delta_vl, 'NLU': avg_delta_nlu, 'NLG': avg_delta_nlg}\n",
    "        performances[f'stage_{i}_avg'] = {'VL': avg_acc_vl, 'NLU': avg_acc_nlu, 'NLG': avg_acc_nlg}\n",
    "    \n",
    "    if not missing_data:\n",
    "        cl_performance_change[model_name] = changes\n",
    "        cl_performance[model_name] = performances\n",
    "\n",
    "# Save the performance changes and averages to JSON files\n",
    "with open('cl_performance_change.json', 'w') as f:\n",
    "    json.dump(cl_performance_change, f, indent=2)\n",
    "\n",
    "with open('cl_performance.json', 'w') as f:\n",
    "    json.dump(cl_performance, f, indent=2)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "name_mapping = {\n",
    "    'sgm': 'SGM',\n",
    "    'sgm-rehearsal1': 'SGM Rehearsal',\n",
    "    'sgm-olf': 'SGM OLF',\n",
    "    'rehearsal1': 'Rehearsal1',\n",
    "    'lora': 'LoRA',\n",
    "    'naive-ft': 'Naive FT',\n",
    "    'soft': 'Soft',\n",
    "    'ia3': 'IA3'\n",
    "}\n",
    "\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
    "  \\\\label{tab:vl_nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{8}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\multicolumn{8}{c}{\\\\textbf{NLU/NLG}} \\\\\\\\\n",
    "     & \\\\multicolumn{2}{c|}{\\\\textbf{Instruct (0)}} & \\\\multicolumn{2}{c|}{\\\\textbf{VQA (1)}} & \\\\multicolumn{2}{c|}{\\\\textbf{OCR (2)}} & \\\\multicolumn{2}{c|}{\\\\textbf{Ref (3)}} & \\\\multicolumn{2}{c|}{\\\\textbf{Instruct (0)}} & \\\\multicolumn{2}{c|}{\\\\textbf{VQA (1)}} & \\\\multicolumn{2}{c|}{\\\\textbf{OCR (2)}} & \\\\multicolumn{2}{c}{\\\\textbf{Ref (3)}} \\\\\\\\\n",
    "     & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} & \\\\textbf{A} & \\\\textbf{$\\\\Delta$} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Include the Naive-FT benchmark first\n",
    "model_name = 'naive-ft'\n",
    "tasks = cl_performance[model_name]\n",
    "\n",
    "model_results = (\n",
    "    name_mapping[model_name],\n",
    "    tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['VL'] * 100,\n",
    "    tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['VL'] * 100,\n",
    "    tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['VL'] * 100,\n",
    "    tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['VL'] * 100,\n",
    "    tasks['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100,\n",
    "    tasks['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100,\n",
    "    tasks['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100,\n",
    "    tasks['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100\n",
    ")\n",
    "\n",
    "latex_code += \"{model} & {t0_vl_a:.1f} & {t0_vl_d:.1f} & {t1_vl_a:.1f} & {t1_vl_d:.1f} & {t2_vl_a:.1f} & {t2_vl_d:.1f} & {t3_vl_a:.1f} & {t3_vl_d:.1f} & {t0_nlu_a:.1f} & {t0_nlu_d:.1f} & {t1_nlu_a:.1f} & {t1_nlu_d:.1f} & {t2_nlu_a:.1f} & {t2_nlu_d:.1f} & {t3_nlu_a:.1f} & {t3_nlu_d:.1f} \\\\\\\\\\n\".format(\n",
    "    model=model_results[0],\n",
    "    t0_vl_a=model_results[1], t0_vl_d=model_results[2],\n",
    "    t1_vl_a=model_results[3], t1_vl_d=model_results[4],\n",
    "    t2_vl_a=model_results[5], t2_vl_d=model_results[6],\n",
    "    t3_vl_a=model_results[7], t3_vl_d=model_results[8],\n",
    "    t0_nlu_a=model_results[9], t0_nlu_d=model_results[10],\n",
    "    t1_nlu_a=model_results[11], t1_nlu_d=model_results[12],\n",
    "    t2_nlu_a=model_results[13], t2_nlu_d=model_results[14],\n",
    "    t3_nlu_a=model_results[15], t3_nlu_d=model_results[16]\n",
    ")\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Include the rest of the models\n",
    "for model_name, tasks in cl_performance.items():\n",
    "    if model_name not in name_mapping or model_name == 'naive-ft':\n",
    "        continue\n",
    "\n",
    "    model_results = (\n",
    "        name_mapping[model_name],\n",
    "        tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['VL'] * 100,\n",
    "        tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['VL'] * 100,\n",
    "        tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['VL'] * 100,\n",
    "        tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['VL'] * 100,\n",
    "        tasks['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100,\n",
    "        tasks['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100,\n",
    "        tasks['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100,\n",
    "        tasks['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100\n",
    "    )\n",
    "    \n",
    "    latex_code += \"{model} & {t0_vl_a:.1f} & {t0_vl_d:.1f} & {t1_vl_a:.1f} & {t1_vl_d:.1f} & {t2_vl_a:.1f} & {t2_vl_d:.1f} & {t3_vl_a:.1f} & {t3_vl_d:.1f} & {t0_nlu_a:.1f} & {t0_nlu_d:.1f} & {t1_nlu_a:.1f} & {t1_nlu_d:.1f} & {t2_nlu_a:.1f} & {t2_nlu_d:.1f} & {t3_nlu_a:.1f} & {t3_nlu_d:.1f} \\\\\\\\\\n\".format(\n",
    "        model=model_results[0],\n",
    "        t0_vl_a=model_results[1], t0_vl_d=model_results[2],\n",
    "        t1_vl_a=model_results[3], t1_vl_d=model_results[4],\n",
    "        t2_vl_a=model_results[5], t2_vl_d=model_results[6],\n",
    "        t3_vl_a=model_results[7], t3_vl_d=model_results[8],\n",
    "        t0_nlu_a=model_results[9], t0_nlu_d=model_results[10],\n",
    "        t1_nlu_a=model_results[11], t1_nlu_d=model_results[12],\n",
    "        t2_nlu_a=model_results[13], t2_nlu_d=model_results[14],\n",
    "        t3_nlu_a=model_results[15], t3_nlu_d=model_results[16]\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
      "  \\label{tab:vl_nlu_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{6}{c|}{\\textbf{Task 0 (Instruct)}} & \\multicolumn{6}{c|}{\\textbf{Task 1 (VQA)}} & \\multicolumn{6}{c|}{\\textbf{Task 2 (OCR)}} & \\multicolumn{6}{c|}{\\textbf{Task 3 (Ref)}} \\\\\n",
      "     & \\multicolumn{2}{c|}{\\textbf{VL}} & \\multicolumn{2}{c|}{\\textbf{NLU}} & \\multicolumn{2}{c|}{\\textbf{NLG}} & \\multicolumn{2}{c|}{\\textbf{VL}} & \\multicolumn{2}{c|}{\\textbf{NLU}} & \\multicolumn{2}{c|}{\\textbf{NLG}} & \\multicolumn{2}{c|}{\\textbf{VL}} & \\multicolumn{2}{c|}{\\textbf{NLU}} & \\multicolumn{2}{c|}{\\textbf{NLG}} & \\multicolumn{2}{c|}{\\textbf{VL}} & \\multicolumn{2}{c|}{\\textbf{NLU}} & \\multicolumn{2}{c|}{\\textbf{NLG}} \\\\\n",
      "     & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$}\\\\\n",
      "     \\midrule\n",
      "Naive FT & 0.3 & 0.2 & 47.9 & 0.4 & 30.3 & -9.1 & 22.2 & 22.1 & 43.9 & -3.6 & 12.4 & -27.0 & 19.5 & 19.4 & 45.3 & -2.2 & 23.2 & -16.1 & 18.4 & 18.3 & 44.8 & -2.7 & 19.4 & -20.0 \\\\\n",
      "\\midrule\n",
      "SGM & 0.0 & -0.1 & 46.6 & -0.9 & 30.6 & -8.7 & 19.2 & 19.1 & 46.0 & -1.5 & 34.3 & -5.1 & 15.4 & 15.3 & 46.6 & -0.9 & 32.2 & -7.2 & 3.8 & 3.7 & 45.1 & -2.4 & 21.9 & -17.5 \\\\\n",
      "SGM OLF & 0.0 & -0.1 & 47.4 & -0.1 & 32.3 & -7.1 & 17.3 & 17.2 & 46.9 & -0.6 & 31.7 & -7.7 & 15.4 & 15.3 & 47.0 & -0.5 & 29.3 & -10.0 & 13.7 & 13.6 & 44.9 & -2.7 & 22.3 & -17.1 \\\\\n",
      "Soft & 0.6 & 0.5 & 47.0 & -0.5 & 32.4 & -7.0 & 0.2 & 0.1 & 43.7 & -3.8 & 10.4 & -29.0 & 14.9 & 14.8 & 46.2 & -1.3 & 22.5 & -16.9 & 1.7 & 1.6 & 44.3 & -3.3 & 13.9 & -25.5 \\\\\n",
      "IA3 & 0.0 & -0.1 & 48.4 & 0.8 & 41.0 & 1.7 & 12.7 & 12.6 & 48.3 & 0.8 & 39.1 & -0.3 & 12.0 & 11.9 & 48.3 & 0.8 & 39.0 & -0.3 & 12.9 & 12.8 & 48.6 & 1.1 & 38.9 & -0.4 \\\\\n",
      "LoRA & 0.2 & 0.1 & 47.0 & -0.5 & 30.7 & -8.6 & 20.4 & 20.3 & 46.2 & -1.3 & 34.7 & -4.7 & 17.5 & 17.4 & 45.7 & -1.8 & 29.7 & -9.7 & 18.1 & 18.0 & 46.7 & -0.8 & 24.0 & -15.4 \\\\\n",
      "Rehearsal1 & 0.3 & 0.2 & 47.9 & 0.4 & 30.3 & -9.1 & 18.7 & 18.6 & 44.0 & -3.5 & 14.0 & -25.4 & 11.1 & 11.0 & 45.5 & -2.0 & 17.8 & -21.6 & 9.8 & 9.7 & 45.2 & -2.3 & 18.3 & -21.1 \\\\\n",
      "SGM Rehearsal & 0.0 & -0.1 & 46.6 & -0.9 & 30.6 & -8.7 & 18.9 & 18.8 & 47.1 & -0.4 & 33.5 & -5.9 & 17.0 & 16.9 & 46.3 & -1.2 & 30.3 & -9.1 & 18.9 & 18.8 & 45.5 & -2.0 & 26.7 & -12.7 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset stages and the corresponding labels\n",
    "stages = [\"Instruct (0)\", \"VQA (1)\", \"OCR (2)\", \"Ref (3)\"]\n",
    "datasets = [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\"]\n",
    "nlu_nlg_datasets = [\"wsc273\", \"winogrande\", \"lambada_standard\", \"arc_easy\", \"arc_challenge\"]\n",
    "vl_datasets = datasets\n",
    "\n",
    "# Define the mitigation methods and their sequence of model names\n",
    "cl_runs = {\n",
    "    \"naive-ft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m\",\n",
    "        \"cl-ocr-stage-2-pythia+410m\",\n",
    "        \"cl-ref-stage-3-pythia+410m\"\n",
    "    ],\n",
    "    \"sgm\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm\"\n",
    "    ],\n",
    "    \"sgm-olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-olf\"\n",
    "    ],\n",
    "    \"soft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-soft\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-soft\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-soft\",\n",
    "        \"cl-ref-stage-3-pythia+410m-soft\"\n",
    "    ],\n",
    "    \"ia3\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-ia3\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-ia3\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-ia3\",\n",
    "        \"cl-ref-stage-3-pythia+410m-ia3\"\n",
    "    ],\n",
    "    \"lora\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-lora\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-lora\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-lora\",\n",
    "        \"cl-ref-stage-3-pythia+410m-lora\"\n",
    "    ],\n",
    "    \"rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-rehearsal1\"\n",
    "    ],\n",
    "    \"sgm-rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-rehearsal1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Baseline run_id\n",
    "baseline_run_id = \"reproduction-align-pythia+410m\"\n",
    "\n",
    "# Load the JSON data from the results file\n",
    "with open('results_CL.json', 'r') as file:\n",
    "    result = json.load(file)\n",
    "\n",
    "# Check for the existence of baseline results\n",
    "baseline_results = result.get(baseline_run_id)\n",
    "if not baseline_results:\n",
    "    raise ValueError(f\"Baseline run ID '{baseline_run_id}' not found in results.\")\n",
    "\n",
    "# Calculate performance changes and averages for each CL run\n",
    "cl_performance_change = {}\n",
    "cl_performance = {}\n",
    "\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    changes = {}\n",
    "    performances = {}\n",
    "    missing_data = False\n",
    "\n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        current_results = result.get(run_id)\n",
    "        if not current_results:\n",
    "            missing_data = True\n",
    "            print(f\"Run '{run_id}' missing for model '{model_name}'\")\n",
    "            break\n",
    "        \n",
    "        change = {dataset: current_results.get(dataset, np.nan) - baseline_results.get(dataset, np.nan) for dataset in baseline_results.keys()}\n",
    "        changes[f'stage_{i}'] = change\n",
    "        performances[f'stage_{i}'] = current_results\n",
    "        \n",
    "        avg_delta_vl = np.nanmean([change[dataset] for dataset in vl_datasets])\n",
    "        avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in vl_datasets])\n",
    "        avg_delta_nlu = np.nanmean([change[dataset] for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_delta_nlg = change[\"lambada_standard\"]\n",
    "        avg_acc_nlg = current_results.get(\"lambada_standard\", np.nan)\n",
    "        \n",
    "        changes[f'stage_{i}_avg'] = {'VL': avg_delta_vl, 'NLU': avg_delta_nlu, 'NLG': avg_delta_nlg}\n",
    "        performances[f'stage_{i}_avg'] = {'VL': avg_acc_vl, 'NLU': avg_acc_nlu, 'NLG': avg_acc_nlg}\n",
    "    \n",
    "    if not missing_data:\n",
    "        cl_performance_change[model_name] = changes\n",
    "        cl_performance[model_name] = performances\n",
    "\n",
    "# Save the performance changes and averages to JSON files\n",
    "with open('cl_performance_change.json', 'w') as f:\n",
    "    json.dump(cl_performance_change, f, indent=2)\n",
    "\n",
    "with open('cl_performance.json', 'w') as f:\n",
    "    json.dump(cl_performance, f, indent=2)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "name_mapping = {\n",
    "    'sgm': 'SGM',\n",
    "    'sgm-rehearsal1': 'SGM Rehearsal',\n",
    "    'sgm-olf': 'SGM OLF',\n",
    "    'rehearsal1': 'Rehearsal1',\n",
    "    'lora': 'LoRA',\n",
    "    'naive-ft': 'Naive FT',\n",
    "    'soft': 'Soft',\n",
    "    'ia3': 'IA3'\n",
    "}\n",
    "\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
    "  \\\\label{tab:vl_nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{6}{c|}{\\\\textbf{Task 0 (Instruct)}} & \\\\multicolumn{6}{c|}{\\\\textbf{Task 1 (VQA)}} & \\\\multicolumn{6}{c|}{\\\\textbf{Task 2 (OCR)}} & \\\\multicolumn{6}{c|}{\\\\textbf{Task 3 (Ref)}} \\\\\\\\\n",
    "     & \\\\multicolumn{2}{c|}{\\\\textbf{VL}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLG}} & \\\\multicolumn{2}{c|}{\\\\textbf{VL}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLG}} & \\\\multicolumn{2}{c|}{\\\\textbf{VL}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLG}} & \\\\multicolumn{2}{c|}{\\\\textbf{VL}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU}} & \\\\multicolumn{2}{c|}{\\\\textbf{NLG}} \\\\\\\\\n",
    "     & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$}\\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Include the Naive-FT benchmark first\n",
    "model_name = 'naive-ft'\n",
    "tasks = cl_performance[model_name]\n",
    "\n",
    "model_results = (\n",
    "    name_mapping[model_name],\n",
    "    tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['VL'] * 100,\n",
    "    tasks['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100,\n",
    "    tasks['stage_0_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLG'] * 100,\n",
    "    tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['VL'] * 100,\n",
    "    tasks['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100,\n",
    "    tasks['stage_1_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLG'] * 100,\n",
    "    tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['VL'] * 100,\n",
    "    tasks['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100,\n",
    "    tasks['stage_2_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLG'] * 100,\n",
    "    tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['VL'] * 100,\n",
    "    tasks['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100,\n",
    "    tasks['stage_3_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLG'] * 100\n",
    ")\n",
    "\n",
    "latex_code += \"{model} & {t0_vl_a:.1f} & {t0_vl_d:.1f} & {t0_nlu_a:.1f} & {t0_nlu_d:.1f} & {t0_nlg_a:.1f} & {t0_nlg_d:.1f} & {t1_vl_a:.1f} & {t1_vl_d:.1f} & {t1_nlu_a:.1f} & {t1_nlu_d:.1f} & {t1_nlg_a:.1f} & {t1_nlg_d:.1f} & {t2_vl_a:.1f} & {t2_vl_d:.1f} & {t2_nlu_a:.1f} & {t2_nlu_d:.1f} & {t2_nlg_a:.1f} & {t2_nlg_d:.1f} & {t3_vl_a:.1f} & {t3_vl_d:.1f} & {t3_nlu_a:.1f} & {t3_nlu_d:.1f} & {t3_nlg_a:.1f} & {t3_nlg_d:.1f} \\\\\\\\\\n\".format(\n",
    "    model=model_results[0],\n",
    "    t0_vl_a=model_results[1], t0_vl_d=model_results[2],\n",
    "    t0_nlu_a=model_results[3], t0_nlu_d=model_results[4],\n",
    "    t0_nlg_a=model_results[5], t0_nlg_d=model_results[6],\n",
    "    t1_vl_a=model_results[7], t1_vl_d=model_results[8],\n",
    "    t1_nlu_a=model_results[9], t1_nlu_d=model_results[10],\n",
    "    t1_nlg_a=model_results[11], t1_nlg_d=model_results[12],\n",
    "    t2_vl_a=model_results[13], t2_vl_d=model_results[14],\n",
    "    t2_nlu_a=model_results[15], t2_nlu_d=model_results[16],\n",
    "    t2_nlg_a=model_results[17], t2_nlg_d=model_results[18],\n",
    "    t3_vl_a=model_results[19], t3_vl_d=model_results[20],\n",
    "    t3_nlu_a=model_results[21], t3_nlu_d=model_results[22],\n",
    "    t3_nlg_a=model_results[23], t3_nlg_d=model_results[24]\n",
    ")\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Include the rest of the models\n",
    "for model_name, tasks in cl_performance.items():\n",
    "    if model_name not in name_mapping or model_name == 'naive-ft':\n",
    "        continue\n",
    "\n",
    "    model_results = (\n",
    "        name_mapping[model_name],\n",
    "        tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['VL'] * 100,\n",
    "        tasks['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100,\n",
    "        tasks['stage_0_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLG'] * 100,\n",
    "        tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['VL'] * 100,\n",
    "        tasks['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100,\n",
    "        tasks['stage_1_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLG'] * 100,\n",
    "        tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['VL'] * 100,\n",
    "        tasks['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100,\n",
    "        tasks['stage_2_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLG'] * 100,\n",
    "        tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['VL'] * 100,\n",
    "        tasks['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100,\n",
    "        tasks['stage_3_avg']['NLG'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLG'] * 100\n",
    "    )\n",
    "    \n",
    "    latex_code += \"{model} & {t0_vl_a:.1f} & {t0_vl_d:.1f} & {t0_nlu_a:.1f} & {t0_nlu_d:.1f} & {t0_nlg_a:.1f} & {t0_nlg_d:.1f} & {t1_vl_a:.1f} & {t1_vl_d:.1f} & {t1_nlu_a:.1f} & {t1_nlu_d:.1f} & {t1_nlg_a:.1f} & {t1_nlg_d:.1f} & {t2_vl_a:.1f} & {t2_vl_d:.1f} & {t2_nlu_a:.1f} & {t2_nlu_d:.1f} & {t2_nlg_a:.1f} & {t2_nlg_d:.1f} & {t3_vl_a:.1f} & {t3_vl_d:.1f} & {t3_nlu_a:.1f} & {t3_nlu_d:.1f} & {t3_nlg_a:.1f} & {t3_nlg_d:.1f} \\\\\\\\\\n\".format(\n",
    "        model=model_results[0],\n",
    "        t0_vl_a=model_results[1], t0_vl_d=model_results[2],\n",
    "        t0_nlu_a=model_results[3], t0_nlu_d=model_results[4],\n",
    "        t0_nlg_a=model_results[5], t0_nlg_d=model_results[6],\n",
    "        t1_vl_a=model_results[7], t1_vl_d=model_results[8],\n",
    "        t1_nlu_a=model_results[9], t1_nlu_d=model_results[10],\n",
    "        t1_nlg_a=model_results[11], t1_nlg_d=model_results[12],\n",
    "        t2_vl_a=model_results[13], t2_vl_d=model_results[14],\n",
    "        t2_nlu_a=model_results[15], t2_nlu_d=model_results[16],\n",
    "        t2_nlg_a=model_results[17], t2_nlg_d=model_results[18],\n",
    "        t3_vl_a=model_results[19], t3_vl_d=model_results[20],\n",
    "        t3_nlu_a=model_results[21], t3_nlu_d=model_results[22],\n",
    "        t3_nlg_a=model_results[23], t3_nlg_d=model_results[24]\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
      "  \\label{tab:vl_nlu_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|ccc|ccc|ccc|ccc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{3}{c|}{\\textbf{Task 0 (Instruct)}} & \\multicolumn{3}{c|}{\\textbf{Task 1 (VQA)}} & \\multicolumn{3}{c|}{\\textbf{Task 2 (OCR)}} & \\multicolumn{3}{c}{\\textbf{Task 3 (Ref)}} \\\\\n",
      "     & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} \\\\\n",
      "     & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{A $\\uparrow$} & \\textbf{$\\Delta \\uparrow$} & \\textbf{$\\Delta \\uparrow$} \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 0.3 & 0.4 & -9.1 & 22.2 & -3.6 & -27.0 & 19.5 & -2.2 & -16.1 & 18.4 & -2.7 & -20.0 \\\\\n",
      "\\midrule\n",
      "OLF & 0.1 & -0.8 & -5.1 & 22.1 & -2.4 & -18.9 & 17.5 & -1.9 & -12.7 & 15.6 & -4.0 & -17.2 \\\\\n",
      "Soft Targets (ST) & 0.6 & -0.5 & -7.0 & 0.2 & -3.8 & -29.0 & 14.9 & -1.3 & -16.9 & 1.7 & -3.3 & -25.5 \\\\\n",
      "IA3 & 0.0 & 0.8 & 1.7 & 12.7 & 0.8 & -0.3 & 12.0 & 0.8 & -0.3 & 12.9 & 1.1 & -0.4 \\\\\n",
      "LoRA & 0.2 & -0.5 & -8.6 & 20.4 & -1.3 & -4.7 & 17.5 & -1.8 & -9.7 & 18.1 & -0.8 & -15.4 \\\\\n",
      "mSGM & 0.0 & -0.9 & -8.7 & 19.2 & -1.5 & -5.1 & 15.4 & -0.9 & -7.2 & 3.8 & -2.4 & -17.5 \\\\\n",
      "mSGM + OLF & 0.0 & -0.1 & -7.1 & 17.3 & -0.6 & -7.7 & 15.4 & -0.5 & -10.0 & 13.7 & -2.7 & -17.1 \\\\\n",
      "Rehearsal \\((1\\%)\\) & 0.3 & 0.4 & -9.1 & 18.7 & -3.5 & -25.4 & 11.1 & -2.0 & -21.6 & 9.8 & -2.3 & -21.1 \\\\\n",
      "mSGM + Rehearsal \\((1\\%)\\) & 0.0 & -0.9 & -8.7 & 18.9 & -0.4 & -5.9 & 17.0 & -1.2 & -9.1 & 18.9 & -2.0 & -12.7 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset stages and the corresponding labels\n",
    "stages = [\"Instruct (0)\", \"VQA (1)\", \"OCR (2)\", \"Ref (3)\"]\n",
    "datasets = [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\"]\n",
    "nlu_nlg_datasets = [\"wsc273\", \"winogrande\", \"lambada_standard\", \"arc_easy\", \"arc_challenge\"]\n",
    "vl_datasets = datasets\n",
    "\n",
    "# Define the mitigation methods and their sequence of model names\n",
    "cl_runs = {\n",
    "    \"naive-ft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m\",\n",
    "        \"cl-ocr-stage-2-pythia+410m\",\n",
    "        \"cl-ref-stage-3-pythia+410m\"\n",
    "    ],\n",
    "    \"olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-olf\"\n",
    "    ],\n",
    "    \"soft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-soft\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-soft\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-soft\",\n",
    "        \"cl-ref-stage-3-pythia+410m-soft\"\n",
    "    ],\n",
    "    \"ia3\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-ia3\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-ia3\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-ia3\",\n",
    "        \"cl-ref-stage-3-pythia+410m-ia3\"\n",
    "    ],\n",
    "    \"lora\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-lora\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-lora\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-lora\",\n",
    "        \"cl-ref-stage-3-pythia+410m-lora\"\n",
    "    ],\n",
    "    \"sgm\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm\"\n",
    "    ],\n",
    "    \"sgm-olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-olf\"\n",
    "    ],\n",
    "    \"rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-rehearsal1\"\n",
    "    ],\n",
    "    \"sgm-rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-rehearsal1\"\n",
    "    ],\n",
    "    \n",
    "}\n",
    "\n",
    "# Baseline run_id\n",
    "baseline_run_id = \"reproduction-align-pythia+410m\"\n",
    "\n",
    "# Load the JSON data from the results file\n",
    "with open('results_CL.json', 'r') as file:\n",
    "    result = json.load(file)\n",
    "\n",
    "# Check for the existence of baseline results\n",
    "baseline_results = result.get(baseline_run_id)\n",
    "if not baseline_results:\n",
    "    raise ValueError(f\"Baseline run ID '{baseline_run_id}' not found in results.\")\n",
    "\n",
    "# Calculate performance changes and averages for each CL run\n",
    "cl_performance_change = {}\n",
    "cl_performance = {}\n",
    "\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    changes = {}\n",
    "    performances = {}\n",
    "    missing_data = False\n",
    "\n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        current_results = result.get(run_id)\n",
    "        if not current_results:\n",
    "            missing_data = True\n",
    "            print(f\"Run '{run_id}' missing for model '{model_name}'\")\n",
    "            break\n",
    "        \n",
    "        change = {dataset: current_results.get(dataset, np.nan) - baseline_results.get(dataset, np.nan) for dataset in baseline_results.keys()}\n",
    "        changes[f'stage_{i}'] = change\n",
    "        performances[f'stage_{i}'] = current_results\n",
    "        \n",
    "        avg_delta_vl = np.nanmean([change[dataset] for dataset in vl_datasets])\n",
    "        avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in vl_datasets])\n",
    "        avg_delta_nlu = np.nanmean([change[dataset] for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_delta_nlg = change[\"lambada_standard\"]\n",
    "        avg_acc_nlg = current_results.get(\"lambada_standard\", np.nan)\n",
    "        \n",
    "        changes[f'stage_{i}_avg'] = {'VL': avg_delta_vl, 'NLU': avg_delta_nlu, 'NLG': avg_delta_nlg}\n",
    "        performances[f'stage_{i}_avg'] = {'VL': avg_acc_vl, 'NLU': avg_acc_nlu, 'NLG': avg_acc_nlg}\n",
    "    \n",
    "    if not missing_data:\n",
    "        cl_performance_change[model_name] = changes\n",
    "        cl_performance[model_name] = performances\n",
    "\n",
    "# Save the performance changes and averages to JSON files\n",
    "with open('cl_performance_change.json', 'w') as f:\n",
    "    json.dump(cl_performance_change, f, indent=2)\n",
    "\n",
    "with open('cl_performance.json', 'w') as f:\n",
    "    json.dump(cl_performance, f, indent=2)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "name_mapping = {\n",
    "    'olf': 'OLF',\n",
    "    'sgm': 'mSGM',\n",
    "    'sgm-rehearsal1': 'mSGM + Rehearsal \\((1\\%)\\)',\n",
    "    'sgm-olf': 'mSGM + OLF',\n",
    "    'rehearsal1': 'Rehearsal \\((1\\%)\\)',\n",
    "    'lora': 'LoRA',\n",
    "    'naive-ft': 'Original LLaVA',\n",
    "    'soft': 'Soft Targets (ST)',\n",
    "    'ia3': 'IA3'\n",
    "}\n",
    "\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
    "  \\\\label{tab:vl_nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|ccc|ccc|ccc|ccc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 0 (Instruct)}} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 1 (VQA)}} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 2 (OCR)}} & \\\\multicolumn{3}{c}{\\\\textbf{Task 3 (Ref)}} \\\\\\\\\n",
    "     & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} \\\\\\\\\n",
    "     & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{A $\\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} & \\\\textbf{$\\\\Delta \\\\uparrow$} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Include the Naive-FT benchmark first\n",
    "model_name = 'naive-ft'\n",
    "tasks = cl_performance[model_name]\n",
    "\n",
    "model_results = (\n",
    "    name_mapping[model_name],\n",
    "    tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLG'] * 100,\n",
    "    tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLG'] * 100,\n",
    "    tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLG'] * 100,\n",
    "    tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLG'] * 100\n",
    ")\n",
    "\n",
    "latex_code += \"{model} & {t0_vl_a:.1f} & {t0_nlu_d:.1f} & {t0_nlg_d:.1f} & {t1_vl_a:.1f} & {t1_nlu_d:.1f} & {t1_nlg_d:.1f} & {t2_vl_a:.1f} & {t2_nlu_d:.1f} & {t2_nlg_d:.1f} & {t3_vl_a:.1f} & {t3_nlu_d:.1f} & {t3_nlg_d:.1f} \\\\\\\\\\n\".format(\n",
    "    model=model_results[0],\n",
    "    t0_vl_a=model_results[1], t0_nlu_d=model_results[2], t0_nlg_d=model_results[3],\n",
    "    t1_vl_a=model_results[4], t1_nlu_d=model_results[5], t1_nlg_d=model_results[6],\n",
    "    t2_vl_a=model_results[7], t2_nlu_d=model_results[8], t2_nlg_d=model_results[9],\n",
    "    t3_vl_a=model_results[10], t3_nlu_d=model_results[11], t3_nlg_d=model_results[12]\n",
    ")\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Include the rest of the models\n",
    "for model_name, tasks in cl_performance.items():\n",
    "    if model_name not in name_mapping or model_name == 'naive-ft':\n",
    "        continue\n",
    "\n",
    "    model_results = (\n",
    "        name_mapping[model_name],\n",
    "        tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLG'] * 100,\n",
    "        tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLG'] * 100,\n",
    "        tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLG'] * 100,\n",
    "        tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLG'] * 100\n",
    "    )\n",
    "    \n",
    "    latex_code += \"{model} & {t0_vl_a:.1f} & {t0_nlu_d:.1f} & {t0_nlg_d:.1f} & {t1_vl_a:.1f} & {t1_nlu_d:.1f} & {t1_nlg_d:.1f} & {t2_vl_a:.1f} & {t2_nlu_d:.1f} & {t2_nlg_d:.1f} & {t3_vl_a:.1f} & {t3_nlu_d:.1f} & {t3_nlg_d:.1f} \\\\\\\\\\n\".format(\n",
    "        model=model_results[0],\n",
    "        t0_vl_a=model_results[1], t0_nlu_d=model_results[2], t0_nlg_d=model_results[3],\n",
    "        t1_vl_a=model_results[4], t1_nlu_d=model_results[5], t1_nlg_d=model_results[6],\n",
    "        t2_vl_a=model_results[7], t2_nlu_d=model_results[8], t2_nlg_d=model_results[9],\n",
    "        t3_vl_a=model_results[10], t3_nlu_d=model_results[11], t3_nlg_d=model_results[12]\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
      "  \\label{tab:vl_nlu_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{3}{c|}{\\textbf{Task 0 (Instruct)}} & \\multicolumn{3}{c|}{\\textbf{Task 1 (VQA)}} & \\multicolumn{3}{c|}{\\textbf{Task 2 (OCR)}} & \\multicolumn{3}{c|}{\\textbf{Task 3 (Ref)}} & \\multicolumn{3}{c}{\\textbf{Avg.}} \\\\\n",
      "     & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} & \\textbf{VL} & \\textbf{NLU} & \\textbf{NLG} \\\\\n",
      "     & \\textbf{(A $\\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{(A $\\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{(A $\\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{(A $\\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{(A $\\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} & \\textbf{($\\Delta \\uparrow$)} \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 0.3 & 0.4 & -9.1 & 22.2 & -3.6 & -27.0 & 19.5 & -2.2 & -16.1 & 18.4 & -2.7 & -20.0 & 15.1 & -2.0 & -18.0 \\\\\n",
      "\\midrule\n",
      "OLF & 0.1 & -0.8 & -5.1 & 22.1 & -2.4 & -18.9 & 17.5 & -1.9 & -12.7 & 15.6 & -4.0 & -17.2 & 13.9 & -2.3 & -13.4 \\\\\n",
      "Soft Targets (ST) & 0.6 & -0.5 & -7.0 & 0.2 & -3.8 & -29.0 & 14.9 & -1.3 & -16.9 & 1.7 & -3.3 & -25.5 & 4.4 & -2.2 & -19.6 \\\\\n",
      "IA3 & 0.0 & 0.8 & 1.7 & 12.7 & 0.8 & -0.3 & 12.0 & 0.8 & -0.3 & 12.9 & 1.1 & -0.4 & 9.4 & 0.9 & 0.2 \\\\\n",
      "LoRA & 0.2 & -0.5 & -8.6 & 20.4 & -1.3 & -4.7 & 17.5 & -1.8 & -9.7 & 18.1 & -0.8 & -15.4 & 14.1 & -1.1 & -9.6 \\\\\n",
      "mSGM & 0.0 & -0.9 & -8.7 & 19.2 & -1.5 & -5.1 & 15.4 & -0.9 & -7.2 & 3.8 & -2.4 & -17.5 & 9.6 & -1.4 & -9.6 \\\\\n",
      "mSGM + OLF & 0.0 & -0.1 & -7.1 & 17.3 & -0.6 & -7.7 & 15.4 & -0.5 & -10.0 & 13.7 & -2.7 & -17.1 & 11.6 & -1.0 & -10.5 \\\\\n",
      "Rehearsal \\((1\\%)\\) & 0.3 & 0.4 & -9.1 & 18.7 & -3.5 & -25.4 & 11.1 & -2.0 & -21.6 & 9.8 & -2.3 & -21.1 & 10.0 & -1.8 & -19.3 \\\\\n",
      "mSGM + Rehearsal \\((1\\%)\\) & 0.0 & -0.9 & -8.7 & 18.9 & -0.4 & -5.9 & 17.0 & -1.2 & -9.1 & 18.9 & -2.0 & -12.7 & 13.7 & -1.1 & -9.1 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset stages and the corresponding labels\n",
    "stages = [\"Instruct (0)\", \"VQA (1)\", \"OCR (2)\", \"Ref (3)\"]\n",
    "datasets = [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\"]\n",
    "nlu_nlg_datasets = [\"wsc273\", \"winogrande\", \"lambada_standard\", \"arc_easy\", \"arc_challenge\"]\n",
    "vl_datasets = datasets\n",
    "\n",
    "# Define the mitigation methods and their sequence of model names\n",
    "cl_runs = {\n",
    "    \"naive-ft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m\",\n",
    "        \"cl-ocr-stage-2-pythia+410m\",\n",
    "        \"cl-ref-stage-3-pythia+410m\"\n",
    "    ],\n",
    "    \"olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-olf\"\n",
    "    ],\n",
    "    \"soft\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-soft\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-soft\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-soft\",\n",
    "        \"cl-ref-stage-3-pythia+410m-soft\"\n",
    "    ],\n",
    "    \"ia3\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-ia3\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-ia3\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-ia3\",\n",
    "        \"cl-ref-stage-3-pythia+410m-ia3\"\n",
    "    ],\n",
    "    \"lora\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-lora\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-lora\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-lora\",\n",
    "        \"cl-ref-stage-3-pythia+410m-lora\"\n",
    "    ],\n",
    "    \"sgm\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm\"\n",
    "    ],\n",
    "    \"sgm-olf\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm-olf\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-olf\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-olf\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-olf\"\n",
    "    ],\n",
    "    \"rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-rehearsal1\"\n",
    "    ],\n",
    "    \"sgm-rehearsal1\": [\n",
    "        \"cl-instruct-stage-0-pythia+410m-sgm\",\n",
    "        \"cl-vqa-stage-1-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ocr-stage-2-pythia+410m-sgm-rehearsal1\",\n",
    "        \"cl-ref-stage-3-pythia+410m-sgm-rehearsal1\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Baseline run_id\n",
    "baseline_run_id = \"reproduction-align-pythia+410m\"\n",
    "\n",
    "# Load the JSON data from the results file\n",
    "with open('results_CL.json', 'r') as file:\n",
    "    result = json.load(file)\n",
    "\n",
    "# Check for the existence of baseline results\n",
    "baseline_results = result.get(baseline_run_id)\n",
    "if not baseline_results:\n",
    "    raise ValueError(f\"Baseline run ID '{baseline_run_id}' not found in results.\")\n",
    "\n",
    "# Calculate performance changes and averages for each CL run\n",
    "cl_performance_change = {}\n",
    "cl_performance = {}\n",
    "\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    changes = {}\n",
    "    performances = {}\n",
    "    missing_data = False\n",
    "\n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        current_results = result.get(run_id)\n",
    "        if not current_results:\n",
    "            missing_data = True\n",
    "            print(f\"Run '{run_id}' missing for model '{model_name}'\")\n",
    "            break\n",
    "        \n",
    "        change = {dataset: current_results.get(dataset, np.nan) - baseline_results.get(dataset, np.nan) for dataset in baseline_results.keys()}\n",
    "        changes[f'stage_{i}'] = change\n",
    "        performances[f'stage_{i}'] = current_results\n",
    "        \n",
    "        avg_delta_vl = np.nanmean([change[dataset] for dataset in vl_datasets])\n",
    "        avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in vl_datasets])\n",
    "        avg_delta_nlu = np.nanmean([change[dataset] for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in nlu_nlg_datasets if dataset != \"lambada_standard\"])\n",
    "        avg_delta_nlg = change[\"lambada_standard\"]\n",
    "        avg_acc_nlg = current_results.get(\"lambada_standard\", np.nan)\n",
    "        \n",
    "        changes[f'stage_{i}_avg'] = {'VL': avg_delta_vl, 'NLU': avg_delta_nlu, 'NLG': avg_delta_nlg}\n",
    "        performances[f'stage_{i}_avg'] = {'VL': avg_acc_vl, 'NLU': avg_acc_nlu, 'NLG': avg_acc_nlg}\n",
    "    \n",
    "    if not missing_data:\n",
    "        cl_performance_change[model_name] = changes\n",
    "        cl_performance[model_name] = performances\n",
    "\n",
    "# Save the performance changes and averages to JSON files\n",
    "with open('cl_performance_change.json', 'w') as f:\n",
    "    json.dump(cl_performance_change, f, indent=2)\n",
    "\n",
    "with open('cl_performance.json', 'w') as f:\n",
    "    json.dump(cl_performance, f, indent=2)\n",
    "\n",
    "# Generate the LaTeX table\n",
    "name_mapping = {\n",
    "    'olf': 'OLF',\n",
    "    'sgm': 'mSGM',\n",
    "    'sgm-rehearsal1': 'mSGM + Rehearsal \\((1\\%)\\)',\n",
    "    'sgm-olf': 'mSGM + OLF',\n",
    "    'rehearsal1': 'Rehearsal \\((1\\%)\\)',\n",
    "    'lora': 'LoRA',\n",
    "    'naive-ft': 'Original LLaVA',\n",
    "    'soft': 'Soft Targets (ST)',\n",
    "    'ia3': 'IA3'\n",
    "}\n",
    "\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{Model Performance:} Task-wise Accuracies and Forgetting of Each Mitigation Method across VL and NLU/NLG tasks}\n",
    "  \\\\label{tab:vl_nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 0 (Instruct)}} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 1 (VQA)}} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 2 (OCR)}} & \\\\multicolumn{3}{c|}{\\\\textbf{Task 3 (Ref)}} & \\\\multicolumn{3}{c}{\\\\textbf{Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} & \\\\textbf{VL} & \\\\textbf{NLU} & \\\\textbf{NLG} \\\\\\\\\n",
    "     & \\\\textbf{(A $\\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{(A $\\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{(A $\\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{(A $\\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{(A $\\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} & \\\\textbf{($\\\\Delta \\\\uparrow$)} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Include the Naive-FT benchmark first\n",
    "model_name = 'naive-ft'\n",
    "tasks = cl_performance[model_name]\n",
    "\n",
    "# Calculate the average values across all tasks\n",
    "avg_vl_a = np.mean([tasks[f'stage_{i}_avg']['VL'] for i in range(4)]) * 100\n",
    "avg_nlu_d = np.mean([cl_performance_change[model_name][f'stage_{i}_avg']['NLU'] for i in range(4)]) * 100\n",
    "avg_nlg_d = np.mean([cl_performance_change[model_name][f'stage_{i}_avg']['NLG'] for i in range(4)]) * 100\n",
    "\n",
    "model_results = (\n",
    "    name_mapping[model_name],\n",
    "    tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLG'] * 100,\n",
    "    tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLG'] * 100,\n",
    "    tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLG'] * 100,\n",
    "    tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLG'] * 100,\n",
    "    avg_vl_a, avg_nlu_d, avg_nlg_d\n",
    ")\n",
    "\n",
    "latex_code += \"{model} & {t0_vl_a:.1f} & {t0_nlu_d:.1f} & {t0_nlg_d:.1f} & {t1_vl_a:.1f} & {t1_nlu_d:.1f} & {t1_nlg_d:.1f} & {t2_vl_a:.1f} & {t2_nlu_d:.1f} & {t2_nlg_d:.1f} & {t3_vl_a:.1f} & {t3_nlu_d:.1f} & {t3_nlg_d:.1f} & {avg_vl_a:.1f} & {avg_nlu_d:.1f} & {avg_nlg_d:.1f} \\\\\\\\\\n\".format(\n",
    "    model=model_results[0],\n",
    "    t0_vl_a=model_results[1], t0_nlu_d=model_results[2], t0_nlg_d=model_results[3],\n",
    "    t1_vl_a=model_results[4], t1_nlu_d=model_results[5], t1_nlg_d=model_results[6],\n",
    "    t2_vl_a=model_results[7], t2_nlu_d=model_results[8], t2_nlg_d=model_results[9],\n",
    "    t3_vl_a=model_results[10], t3_nlu_d=model_results[11], t3_nlg_d=model_results[12],\n",
    "    avg_vl_a=model_results[13], avg_nlu_d=model_results[14], avg_nlg_d=model_results[15]\n",
    ")\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "# Include the rest of the models\n",
    "for model_name, tasks in cl_performance.items():\n",
    "    if model_name not in name_mapping or model_name == 'naive-ft':\n",
    "        continue\n",
    "\n",
    "    # Calculate the average values across all tasks\n",
    "    avg_vl_a = np.mean([tasks[f'stage_{i}_avg']['VL'] for i in range(4)]) * 100\n",
    "    avg_nlu_d = np.mean([cl_performance_change[model_name][f'stage_{i}_avg']['NLU'] for i in range(4)]) * 100\n",
    "    avg_nlg_d = np.mean([cl_performance_change[model_name][f'stage_{i}_avg']['NLG'] for i in range(4)]) * 100\n",
    "\n",
    "    model_results = (\n",
    "        name_mapping[model_name],\n",
    "        tasks['stage_0_avg']['VL'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_0_avg']['NLG'] * 100,\n",
    "        tasks['stage_1_avg']['VL'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_1_avg']['NLG'] * 100,\n",
    "        tasks['stage_2_avg']['VL'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_2_avg']['NLG'] * 100,\n",
    "        tasks['stage_3_avg']['VL'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLU'] * 100, cl_performance_change[model_name]['stage_3_avg']['NLG'] * 100,\n",
    "        avg_vl_a, avg_nlu_d, avg_nlg_d\n",
    "    )\n",
    "    \n",
    "    latex_code += \"{model} & {t0_vl_a:.1f} & {t0_nlu_d:.1f} & {t0_nlg_d:.1f} & {t1_vl_a:.1f} & {t1_nlu_d:.1f} & {t1_nlg_d:.1f} & {t2_vl_a:.1f} & {t2_nlu_d:.1f} & {t2_nlg_d:.1f} & {t3_vl_a:.1f} & {t3_nlu_d:.1f} & {t3_nlg_d:.1f} & {avg_vl_a:.1f} & {avg_nlu_d:.1f} & {avg_nlg_d:.1f} \\\\\\\\\\n\".format(\n",
    "        model=model_results[0],\n",
    "        t0_vl_a=model_results[1], t0_nlu_d=model_results[2], t0_nlg_d=model_results[3],\n",
    "        t1_vl_a=model_results[4], t1_nlu_d=model_results[5], t1_nlg_d=model_results[6],\n",
    "        t2_vl_a=model_results[7], t2_nlu_d=model_results[8], t2_nlg_d=model_results[9],\n",
    "        t3_vl_a=model_results[10], t3_nlu_d=model_results[11], t3_nlg_d=model_results[12],\n",
    "        avg_vl_a=model_results[13], avg_nlu_d=model_results[14], avg_nlg_d=model_results[15]\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
