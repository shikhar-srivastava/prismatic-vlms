{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You're given a results_nlp.json of the format below:\n",
    "# {\n",
    "#   \"stage-final-llava-v15-phi1_5+1b\": {\n",
    "#     \"vqa-v2\": 0.0,\n",
    "#     \"textvqa-ocr\": 0.0,\n",
    "#     \"textvqa-pure\": 0.0,\n",
    "#     \"gqa\": 0.0,\n",
    "#     \"refcoco\": 0.0,\n",
    "#     \"wsc273\": 0.8095238095238095,\n",
    "#     \"winogrande\": 0.7245461720599842,\n",
    "#     \"lambada_standard\": 0.3114690471569959,\n",
    "#     \"arc_easy\": 0.742003367003367,\n",
    "#     \"arc_challenge\": 0.40102389078498296\n",
    "#   },\n",
    "#   \"stage-final-llava-v15-pythia+1p4b-sgm-old\": {\n",
    "#     \"vqa-v2\": 0.5317000000000001,\n",
    "#     \"textvqa-ocr\": 0.28222656250000006,\n",
    "#     \"textvqa-pure\": 0.23583984375000014,\n",
    "#     \"gqa\": 0.36619999999999997,\n",
    "#     \"refcoco\": 0.0234375,\n",
    "#     \"wsc273\": 0.717948717948718,\n",
    "#     \"winogrande\": 0.5737963693764798,\n",
    "#     \"lambada_standard\": 0.4513875412381137,\n",
    "#     \"arc_easy\": 0.6014309764309764,\n",
    "#     \"arc_challenge\": 0.2841296928327645\n",
    "#   },\n",
    "#   \"stage-final-llava-v15-pythia+160m-ptune\": {\n",
    "#     \"vqa-v2\": 0.0,\n",
    "#     \"textvqa-ocr\": 0.0,\n",
    "#     \"textvqa-pure\": 0.0,\n",
    "#     \"gqa\": 0.0,\n",
    "#     \"refcoco\": 0.0\n",
    "#   },\n",
    "# }\n",
    "\n",
    "# We are concerned with the following models:\n",
    "# pythia+1p4b and pythia+1p4b-instruct: Respectively:\n",
    "# \treproduction-align-pythia+1p4b and reproduction-align-pythia+1p4b-instruct for their language only LLMs method\n",
    "# \tstage-final-llava-v15-pythia+1p4b and stage-final-llava-v15-pythia+1p4b-instruct for their Original LLaVA method\n",
    "# LLaMA2 7B and Vicuna1.5 7B:\n",
    "\t\n",
    "# \tand reproduction-llava-v15+7b+stage-finetune+x7 for the Original LLaVA method\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the datasets of interest\n",
    "datasets = [\n",
    "    'vqa-v2', 'textvqa-ocr', 'textvqa-pure', 'gqa', 'refcoco',\n",
    "    'wsc273', 'winogrande', 'lambada_standard', 'arc_easy', 'arc_challenge'\n",
    "]\n",
    "\n",
    "# Define the result dictionary\n",
    "result = {}\n",
    "\n",
    "# Path to the JSON results file\n",
    "results_file = 'results_nlp.json'\n",
    "\n",
    "# Read the JSON results file\n",
    "if os.path.isfile(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        result = json.load(f)\n",
    "else:\n",
    "    print(f\"Error: File {results_file} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "original_llava_acc = results_dict[\"Original LLaVA\"]\n",
    "language_only_llm_acc = results_dict.get(\"Language Only LLM\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model in [\"Original LLaVA\", \"Language Only LLM\"]:\n",
    "        continue\n",
    "    deltas = {dataset: acc - original_llava_acc[dataset] for dataset, acc in accuracies.items()}\n",
    "    avg_delta_vl = sum(deltas[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    avg_delta_nlu = sum(deltas[dataset] for dataset in accuracies if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]) / 5\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]) / 5\n",
    "    table_data.append((model, accuracies, deltas, avg_delta_vl, avg_acc_vl, avg_delta_nlu, avg_acc_nlu))\n",
    "\n",
    "# Sort the data so models with smallest average delta VL are first\n",
    "table_data = sorted(table_data, key=lambda x: x[3])\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Generate LaTeX table for Vision-Language part\n",
    "latex_code_vl = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Vision-Language (VL) Tasks}\n",
    "  \\\\label{tab:vl_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\multicolumn{2}{c}{\\\\textbf{VL Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & $\\\\Delta$ & Acc \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code_vl += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & - & {avg_acc_vl} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    avg_acc_vl=format_value((original_llava_acc[\"vqa-v2\"] + original_llava_acc[\"textvqa-ocr\"] + original_llava_acc[\"textvqa-pure\"] + original_llava_acc[\"gqa\"]) / 4)\n",
    ")\n",
    "\n",
    "latex_code_vl += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & - & {avg_acc_vl} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    avg_acc_vl=format_value((language_only_llm_acc.get(\"vqa-v2\", 0) + language_only_llm_acc.get(\"textvqa-ocr\", 0) + language_only_llm_acc.get(\"textvqa-pure\", 0) + language_only_llm_acc.get(\"gqa\", 0)) / 4)\n",
    ")\n",
    "\n",
    "latex_code_vl += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, deltas, avg_delta_vl, avg_acc_vl, _, _ in table_data:\n",
    "    latex_code_vl += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {delta_vl} & {acc_vl} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        delta_vl=format_value(avg_delta_vl),\n",
    "        acc_vl=format_value(avg_acc_vl)\n",
    "    )\n",
    "\n",
    "latex_code_vl += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code_vl)\n",
    "\n",
    "# Generate LaTeX table for NLU/NLG part\n",
    "latex_code_nlu = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} NLU/NLG Tasks}\n",
    "  \\\\label{tab:nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|ccccc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{5}{c|}{\\\\textbf{NLU/NLG}} & \\\\multicolumn{2}{c}{\\\\textbf{NLU Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{WSC273} & \\\\textbf{Winogrande} & \\\\textbf{ARC Easy} & \\\\textbf{ARC Challenge} & \\\\textbf{Lambada} & $\\\\Delta$ & Acc \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code_nlu += \"Original LLaVA & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} & - & {avg_acc_nlu} \\\\\\\\\\n\".format(\n",
    "    wsc273=format_value(original_llava_acc[\"wsc273\"]),\n",
    "    winogrande=format_value(original_llava_acc[\"winogrande\"]),\n",
    "    arc_easy=format_value(original_llava_acc[\"arc_easy\"]),\n",
    "    arc_challenge=format_value(original_llava_acc[\"arc_challenge\"]),\n",
    "    lambada=format_value(original_llava_acc[\"lambada_standard\"]),\n",
    "    avg_acc_nlu=format_value((original_llava_acc[\"wsc273\"] + original_llava_acc[\"winogrande\"] + original_llava_acc[\"arc_easy\"] + original_llava_acc[\"arc_challenge\"] + original_llava_acc[\"lambada_standard\"]) / 5)\n",
    ")\n",
    "\n",
    "latex_code_nlu += \"Language Only LLM & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} & - & {avg_acc_nlu} \\\\\\\\\\n\".format(\n",
    "    wsc273=format_value(language_only_llm_acc.get(\"wsc273\", np.nan)),\n",
    "    winogrande=format_value(language_only_llm_acc.get(\"winogrande\", np.nan)),\n",
    "    arc_easy=format_value(language_only_llm_acc.get(\"arc_easy\", np.nan)),\n",
    "    arc_challenge=format_value(language_only_llm_acc.get(\"arc_challenge\", np.nan)),\n",
    "    lambada=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan)),\n",
    "    avg_acc_nlu=format_value((language_only_llm_acc.get(\"wsc273\", 0) + language_only_llm_acc.get(\"winogrande\", 0) + language_only_llm_acc.get(\"arc_easy\", 0) + language_only_llm_acc.get(\"arc_challenge\", 0) + language_only_llm_acc.get(\"lambada_standard\", 0)) / 5)\n",
    ")\n",
    "\n",
    "latex_code_nlu += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, deltas, _, _, avg_delta_nlu, avg_acc_nlu in table_data:\n",
    "    latex_code_nlu += \"{model} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} & {delta_nlu} & {acc_nlu} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        wsc273=format_value(accuracies[\"wsc273\"]),\n",
    "        winogrande=format_value(accuracies[\"winogrande\"]),\n",
    "        arc_easy=format_value(accuracies[\"arc_easy\"]),\n",
    "        arc_challenge=format_value(accuracies[\"arc_challenge\"]),\n",
    "        lambada=format_value(accuracies[\"lambada_standard\"]),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        acc_nlu=format_value(avg_acc_nlu)\n",
    "    )\n",
    "\n",
    "latex_code_nlu += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code_nlu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance:} LoRA Variants}\n",
      "  \\label{tab:lora_variants_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccc|c|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{4}{c|}{\\textbf{Vision-Language (VL)}} & \\textbf{VL Avg.} & \\multicolumn{2}{c|}{\\textbf{NLU Avg.}} & \\multicolumn{2}{c}{\\textbf{NLG Avg.}} \\\\\n",
      "     & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 30.3 & 2.4 & 3.8 & 22.2 & 14.7 & -1.5 & 41.1 & -12.0 & 11.3 \\\\\n",
      "Language Only LLM & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & - & 42.7 & - & 23.3 \\\\\n",
      "\\midrule\n",
      "LoRA (1/2 Full Rank, Higher Alpha) & 28.7 & 1.1 & 2.7 & 19.7 & 13.0 & -1.0 & 41.6 & -13.2 & 10.1 \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA) & 29.0 & 1.0 & 1.7 & 18.0 & 12.4 & -1.5 & 41.2 & -3.9 & 19.5 \\\\\n",
      "LoRA (1/4 Full Rank) & 24.6 & 0.9 & 1.4 & 15.0 & 10.5 & -2.6 & 40.0 & -15.2 & 8.1 \\\\\n",
      "LoRA (1/4 Full Rank, Higher Alpha) & 6.5 & 0.7 & 0.5 & 2.4 & 2.5 & -2.0 & 40.7 & -4.6 & 18.7 \\\\\n",
      "LoRA (1/2 Full Rank) & 0.1 & 0.2 & 0.1 & 0.0 & 0.1 & - & - & - & - \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA, KQV Target) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & - & - & - & - \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance:} Other Methods}\n",
      "  \\label{tab:other_methods_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccc|c|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{4}{c|}{\\textbf{Vision-Language (VL)}} & \\textbf{VL Avg.} & \\multicolumn{2}{c|}{\\textbf{NLU Avg.}} & \\multicolumn{2}{c}{\\textbf{NLG Avg.}} \\\\\n",
      "     & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 30.3 & 2.4 & 3.8 & 22.2 & 14.7 & -1.5 & 41.1 & -12.0 & 11.3 \\\\\n",
      "Language Only LLM & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & - & 42.7 & - & 23.3 \\\\\n",
      "\\midrule\n",
      "Soft Targets & 32.7 & 6.9 & 6.1 & 25.4 & 17.8 & -1.6 & 41.1 & -6.3 & 17.0 \\\\\n",
      "SGM & 28.4 & 1.4 & 2.7 & 17.5 & 12.5 & -0.6 & 42.1 & -5.6 & 17.7 \\\\\n",
      "LoRA & 29.0 & 1.0 & 1.7 & 18.0 & 12.4 & -1.5 & 41.2 & -3.9 & 19.5 \\\\\n",
      "Output Layer Freezing (OLF) & 26.9 & 0.7 & 3.6 & 17.8 & 12.2 & -2.2 & 40.4 & -7.9 & 15.4 \\\\\n",
      "Soft Targets + OLF & 21.1 & 2.5 & 2.6 & 15.4 & 10.4 & -0.6 & 42.1 & -3.6 & 19.7 \\\\\n",
      "SGM + OLF & 1.0 & 1.2 & 0.2 & 0.8 & 0.8 & -0.5 & 42.1 & -2.3 & 21.0 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "original_llava_acc = results_dict[\"Original LLaVA\"]\n",
    "language_only_llm_acc = results_dict.get(\"Language Only LLM\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model in [\"Original LLaVA\", \"Language Only LLM\"]:\n",
    "        continue\n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    nlu_deltas = {dataset: accuracies[dataset] - language_only_llm_acc.get(dataset, 0) for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]}\n",
    "    avg_delta_nlu = sum(nlu_deltas[dataset] for dataset in nlu_deltas) / 4\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in nlu_deltas) / 4\n",
    "    delta_nlg = accuracies[\"lambada_standard\"] - language_only_llm_acc.get(\"lambada_standard\", 0)\n",
    "    avg_acc_nlg = accuracies[\"lambada_standard\"]\n",
    "    table_data.append((model, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg))\n",
    "\n",
    "# Sort the data by Avg. VL Accuracy and highest NLG Delta\n",
    "table_data = sorted(table_data, key=lambda x: (x[2], -x[5]), reverse=True)\n",
    "\n",
    "# Separate the data into two groups\n",
    "lora_variants = [\n",
    "    'LoRA (1/4 Full Rank)', 'LoRA (1/4 Full Rank, Higher Alpha)', \n",
    "    'LoRA (1/2 Full Rank)', 'LoRA (1/2 Full Rank, Higher Alpha)', \n",
    "    'LoRA (1/2 Full Rank, RSLoRA)', 'LoRA (1/2 Full Rank, RSLoRA, KQV Target)'\n",
    "]\n",
    "\n",
    "lora_table_data = [item for item in table_data if item[0] in lora_variants]\n",
    "other_table_data = [item for item in table_data if item[0] not in lora_variants]\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Generate LaTeX table for LoRA variants\n",
    "latex_code_lora = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} LoRA Variants}\n",
    "  \\\\label{tab:lora_variants_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|c|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\textbf{VL Avg.} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NLG Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "original_llava_deltas = {dataset: original_llava_acc[dataset] - language_only_llm_acc.get(dataset, 0) for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]}\n",
    "original_llava_avg_delta_nlu = sum(original_llava_deltas[dataset] for dataset in original_llava_deltas) / 4\n",
    "original_llava_delta_nlg = original_llava_acc[\"lambada_standard\"] - language_only_llm_acc.get(\"lambada_standard\", 0)\n",
    "\n",
    "latex_code_lora += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & {delta_nlu} & {avg_acc_nlu} & {delta_nlg} & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    avg_acc_vl=format_value((original_llava_acc[\"vqa-v2\"] + original_llava_acc[\"textvqa-ocr\"] + original_llava_acc[\"textvqa-pure\"] + original_llava_acc[\"gqa\"]) / 4),\n",
    "    delta_nlu=format_value(original_llava_avg_delta_nlu),\n",
    "    avg_acc_nlu=format_value((original_llava_acc[\"wsc273\"] + original_llava_acc[\"winogrande\"] + original_llava_acc[\"arc_easy\"] + original_llava_acc[\"arc_challenge\"]) / 4),\n",
    "    delta_nlg=format_value(original_llava_delta_nlg),\n",
    "    avg_acc_nlg=format_value(original_llava_acc[\"lambada_standard\"])\n",
    ")\n",
    "\n",
    "latex_code_lora += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & - & {avg_acc_nlu} & - & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    avg_acc_vl=format_value((language_only_llm_acc.get(\"vqa-v2\", 0) + language_only_llm_acc.get(\"textvqa-ocr\", 0) + language_only_llm_acc.get(\"textvqa-pure\", 0) + language_only_llm_acc.get(\"gqa\", 0)) / 4),\n",
    "    avg_acc_nlu=format_value((language_only_llm_acc.get(\"wsc273\", 0) + language_only_llm_acc.get(\"winogrande\", 0) + language_only_llm_acc.get(\"arc_easy\", 0) + language_only_llm_acc.get(\"arc_challenge\", 0)) / 4),\n",
    "    avg_acc_nlg=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan))\n",
    ")\n",
    "\n",
    "latex_code_lora += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg in lora_table_data:\n",
    "    latex_code_lora += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {acc_vl} & {delta_nlu} & {acc_nlu} & {delta_nlg} & {acc_nlg} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        acc_vl=format_value(avg_acc_vl),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        acc_nlu=format_value(avg_acc_nlu),\n",
    "        delta_nlg=format_value(delta_nlg),\n",
    "        acc_nlg=format_value(avg_acc_nlg)\n",
    "    )\n",
    "\n",
    "latex_code_lora += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "# Generate LaTeX table for other methods\n",
    "latex_code_other = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Other Methods}\n",
    "  \\\\label{tab:other_methods_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|c|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\textbf{VL Avg.} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NLG Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code_other += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & {delta_nlu} & {avg_acc_nlu} & {delta_nlg} & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    avg_acc_vl=format_value((original_llava_acc[\"vqa-v2\"] + original_llava_acc[\"textvqa-ocr\"] + original_llava_acc[\"textvqa-pure\"] + original_llava_acc[\"gqa\"]) / 4),\n",
    "    delta_nlu=format_value(original_llava_avg_delta_nlu),\n",
    "    avg_acc_nlu=format_value((original_llava_acc[\"wsc273\"] + original_llava_acc[\"winogrande\"] + original_llava_acc[\"arc_easy\"] + original_llava_acc[\"arc_challenge\"]) / 4),\n",
    "    delta_nlg=format_value(original_llava_delta_nlg),\n",
    "    avg_acc_nlg=format_value(original_llava_acc[\"lambada_standard\"])\n",
    ")\n",
    "\n",
    "latex_code_other += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & - & {avg_acc_nlu} & - & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    avg_acc_vl=format_value((language_only_llm_acc.get(\"vqa-v2\", 0) + language_only_llm_acc.get(\"textvqa-ocr\", 0) + language_only_llm_acc.get(\"textvqa-pure\", 0) + language_only_llm_acc.get(\"gqa\", 0)) / 4),\n",
    "    avg_acc_nlu=format_value((language_only_llm_acc.get(\"wsc273\", 0) + language_only_llm_acc.get(\"winogrande\", 0) + language_only_llm_acc.get(\"arc_easy\", 0) + language_only_llm_acc.get(\"arc_challenge\", 0)) / 4),\n",
    "    avg_acc_nlg=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan))\n",
    ")\n",
    "\n",
    "latex_code_other += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg in other_table_data:\n",
    "    latex_code_other += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {acc_vl} & {delta_nlu} & {acc_nlu} & {delta_nlg} & {acc_nlg} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        acc_vl=format_value(avg_acc_vl),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        acc_nlu=format_value(avg_acc_nlu),\n",
    "        delta_nlg=format_value(delta_nlg),\n",
    "        acc_nlg=format_value(avg_acc_nlg)\n",
    "    )\n",
    "\n",
    "latex_code_other += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code_lora)\n",
    "print(latex_code_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance:} Per Dataset}\n",
      "  \\label{tab:per_dataset}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccccccccc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & \\textbf{WSC273} & \\textbf{Winogrande} & \\textbf{ARC Easy} & \\textbf{ARC Challenge} & \\textbf{Lambada} \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 30.3 & 2.4 & 3.8 & 22.2 & 53.5 & 51.9 & 39.1 & 20.1 & 11.3 \\\\\n",
      "Language Only LLM & 0.0 & 0.0 & 0.0 & 0.0 & 56.8 & 49.6 & 44.2 & 20.0 & 23.3 \\\\\n",
      "\\midrule\n",
      "LoRA & 29.0 & 1.0 & 1.7 & 18.0 & 54.6 & 49.6 & 38.9 & 21.7 & 19.5 \\\\\n",
      "LoRA (1/2 Full Rank) & 0.1 & 0.2 & 0.1 & 0.0 & - & - & - & - & - \\\\\n",
      "LoRA (1/2 Full Rank, Higher Alpha) & 28.7 & 1.1 & 2.7 & 19.7 & 58.6 & 50.4 & 38.7 & 18.7 & 10.1 \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA) & 29.0 & 1.0 & 1.7 & 18.0 & 54.6 & 49.6 & 38.9 & 21.7 & 19.5 \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA, KQV Target) & 0.0 & 0.0 & 0.0 & 0.0 & - & - & - & - & - \\\\\n",
      "LoRA (1/4 Full Rank) & 24.6 & 0.9 & 1.4 & 15.0 & 53.1 & 50.1 & 36.5 & 20.4 & 8.1 \\\\\n",
      "LoRA (1/4 Full Rank, Higher Alpha) & 6.5 & 0.7 & 0.5 & 2.4 & 53.8 & 49.7 & 38.1 & 20.9 & 18.7 \\\\\n",
      "Output Layer Freezing (OLF) & 26.9 & 0.7 & 3.6 & 17.8 & 52.4 & 53.4 & 36.2 & 19.6 & 15.4 \\\\\n",
      "SGM & 28.4 & 1.4 & 2.7 & 17.5 & 56.8 & 50.7 & 40.0 & 20.7 & 17.7 \\\\\n",
      "SGM + OLF & 1.0 & 1.2 & 0.2 & 0.8 & 56.0 & 50.6 & 41.6 & 20.2 & 21.0 \\\\\n",
      "Soft Targets & 32.7 & 6.9 & 6.1 & 25.4 & 51.3 & 50.8 & 39.9 & 22.3 & 17.0 \\\\\n",
      "Soft Targets + OLF & 21.1 & 2.5 & 2.6 & 15.4 & 53.1 & 51.6 & 40.4 & 23.2 & 19.7 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "original_llava_acc = results_dict[\"Original LLaVA\"]\n",
    "language_only_llm_acc = results_dict.get(\"Language Only LLM\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model not in [\"Original LLaVA\", \"Language Only LLM\"]:\n",
    "        table_data.append((model, accuracies))\n",
    "\n",
    "# Sort the data by model name\n",
    "table_data = sorted(table_data, key=lambda x: x[0])\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Per Dataset}\n",
    "  \\\\label{tab:per_dataset}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccccccccc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & \\\\textbf{WSC273} & \\\\textbf{Winogrande} & \\\\textbf{ARC Easy} & \\\\textbf{ARC Challenge} & \\\\textbf{Lambada} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    wsc273=format_value(original_llava_acc[\"wsc273\"]),\n",
    "    winogrande=format_value(original_llava_acc[\"winogrande\"]),\n",
    "    arc_easy=format_value(original_llava_acc[\"arc_easy\"]),\n",
    "    arc_challenge=format_value(original_llava_acc[\"arc_challenge\"]),\n",
    "    lambada=format_value(original_llava_acc[\"lambada_standard\"])\n",
    ")\n",
    "\n",
    "latex_code += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    wsc273=format_value(language_only_llm_acc.get(\"wsc273\", np.nan)),\n",
    "    winogrande=format_value(language_only_llm_acc.get(\"winogrande\", np.nan)),\n",
    "    arc_easy=format_value(language_only_llm_acc.get(\"arc_easy\", np.nan)),\n",
    "    arc_challenge=format_value(language_only_llm_acc.get(\"arc_challenge\", np.nan)),\n",
    "    lambada=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan))\n",
    ")\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies in table_data:\n",
    "    latex_code += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        wsc273=format_value(accuracies[\"wsc273\"]),\n",
    "        winogrande=format_value(accuracies[\"winogrande\"]),\n",
    "        arc_easy=format_value(accuracies[\"arc_easy\"]),\n",
    "        arc_challenge=format_value(accuracies[\"arc_challenge\"]),\n",
    "        lambada=format_value(accuracies[\"lambada_standard\"])\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
