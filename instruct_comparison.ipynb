{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the datasets of interest\n",
    "datasets = [\n",
    "    'vqa-v2', 'textvqa-ocr', 'textvqa-pure', 'gqa', 'refcoco',\n",
    "    'wsc273', 'winogrande', 'lambada_standard', 'arc_easy', 'arc_challenge'\n",
    "]\n",
    "\n",
    "# Define the result dictionary\n",
    "results_nlp = {}\n",
    "\n",
    "# Path to the JSON results file\n",
    "results_file = 'results_nlp.json'\n",
    "\n",
    "# Read the JSON results file\n",
    "if os.path.isfile(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        results_nlp = json.load(f)\n",
    "else:\n",
    "    print(f\"Error: File {results_file} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance}}\n",
      "  \\label{tab:model_performance}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccc|c|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{4}{c|}{\\textbf{Vision-Language (VL)}} & \\textbf{VL Avg.} & \\multicolumn{2}{c|}{\\textbf{NLU Avg.}} & \\multicolumn{2}{c}{\\textbf{NLG Avg.}} \\\\\n",
      "     & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ \\\\\n",
      "     \\midrule\n",
      "LLaVA + LLaMA2 Base (7B) & 75.9 & 55.2 & 45.4 & 60.2 & 59.2 & 2.7 & 70.0 & 0.4 & 68.7 \\\\\n",
      "LLaVA + LLaMA2 Instruct (7B) & 74.5 & 56.3 & 45.9 & 56.2 & 58.2 & 0.3 & 68.8 & -2.0 & 62.3 \\\\\n",
      "LLaVA + Pythia Instruct (1.4B) & 66.2 & 38.5 & 35.5 & 46.1 & 46.6 & -1.1 & 53.0 & -8.1 & 40.9 \\\\\n",
      "LLaVA + Pythia (1.4B) & 64.0 & 39.8 & 34.4 & 44.5 & 45.7 & -4.6 & 49.5 & -12.6 & 36.3 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the results from the JSON file\n",
    "with open('results_nlp.json') as f:\n",
    "    results_dict = json.load(f)\n",
    "\n",
    "# Define the models of interest and their corresponding baselines with labels\n",
    "models_of_interest = {\n",
    "    \"stage-final-llava-v15-pythia+1p4b\": (\"reproduction-align-pythia+1p4b\", \"LLaVA + Pythia Instruct (1.4B)\"),\n",
    "    \"stage-final-llava-v15-pythia+1p4b-instruct-old\": (\"reproduction-align-pythia+1p4b-instruct-old\", \"LLaVA + Pythia (1.4B)\"),\n",
    "    \"reproduction-llava-v15+7b+stage-finetune+x7\": (\"reproduction-llava-v15+7b+stage-align+x7\", \"LLaVA + LLaMA2 Instruct (7B)\"),\n",
    "    \"reproduction-llama2\": (\"vila_base_llm\", \"LLaVA + LLaMA2 Base (7B)\")\n",
    "}\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Prepare the data for the LaTeX tables\n",
    "table_data = []\n",
    "\n",
    "for model, (baseline, label) in models_of_interest.items():\n",
    "    accuracies = results_dict[model]\n",
    "    baseline_accuracies = results_dict[baseline]\n",
    "    \n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    \n",
    "    nlu_deltas = {dataset: accuracies[dataset] - baseline_accuracies.get(dataset, 0) for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]}\n",
    "    avg_delta_nlu = sum(nlu_deltas[dataset] for dataset in nlu_deltas) / 4\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in nlu_deltas) / 4\n",
    "    \n",
    "    delta_nlg = accuracies[\"lambada_standard\"] - baseline_accuracies.get(\"lambada_standard\", 0)\n",
    "    avg_acc_nlg = accuracies[\"lambada_standard\"]\n",
    "    \n",
    "    table_data.append((label, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg))\n",
    "\n",
    "# Sort the data by Avg. VL Accuracy and highest NLG Delta\n",
    "table_data = sorted(table_data, key=lambda x: (x[2], -x[5]), reverse=True)\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance}}\n",
    "  \\\\label{tab:model_performance}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|c|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\textbf{VL Avg.} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NLG Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for label, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg in table_data:\n",
    "    latex_code += \"{label} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & {delta_nlu} & {avg_acc_nlu} & {delta_nlg} & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "        label=label,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        avg_acc_vl=format_value(avg_acc_vl),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        avg_acc_nlu=format_value(avg_acc_nlu),\n",
    "        delta_nlg=format_value(delta_nlg),\n",
    "        avg_acc_nlg=format_value(avg_acc_nlg)\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance}}\n",
      "  \\label{tab:model_performance}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccc|c|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{4}{c|}{\\textbf{Vision-Language (VL)}} & \\textbf{VL Avg.} & \\multicolumn{2}{c|}{\\textbf{NLU Avg.}} & \\multicolumn{2}{c}{\\textbf{NLG Avg.}} \\\\\n",
      "     & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ \\\\\n",
      "     \\midrule\n",
      "LLaVA + LLaMA2 Base (7B) & 75.9 & 55.2 & 45.4 & 60.2 & 59.2 & 2.7 & 70.0 & 0.4 & 68.7 \\\\\n",
      "LLaVA + LLaMA2 Instruct (7B) & 74.5 & 56.3 & 45.9 & 56.2 & 58.2 & 0.3 & 68.8 & -2.0 & 62.3 \\\\\n",
      "LLaVA + Pythia (1.4B) & 66.5 & 39.1 & 34.3 & 46.9 & 46.7 & 2.9 & 49.1 & 1.0 & 34.8 \\\\\n",
      "LLaVA + Pythia Instruct (1.4B) & 66.2 & 38.5 & 35.5 & 46.1 & 46.6 & -1.1 & 53.0 & -8.1 & 40.9 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the results from the JSON file\n",
    "with open('results_nlp.json') as f:\n",
    "    results_dict = json.load(f)\n",
    "\n",
    "# Define the models of interest and their corresponding baselines with labels\n",
    "models_of_interest = {\n",
    "    \"stage-final-llava-v15-pythia+1p4b\": (\"reproduction-align-pythia+1p4b\", \"LLaVA + Pythia Instruct (1.4B)\"),\n",
    "    \"stage-final-llava-v15-pythia+1p4b-instruct\": (\"reproduction-align-pythia+1p4b-instruct\", \"LLaVA + Pythia (1.4B)\"),\n",
    "    \"reproduction-llava-v15+7b+stage-finetune+x7\": (\"reproduction-llava-v15+7b+stage-align+x7\", \"LLaVA + LLaMA2 Instruct (7B)\"),\n",
    "    \"reproduction-llama2\": (\"vila_base_llm\", \"LLaVA + LLaMA2 Base (7B)\")\n",
    "}\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Prepare the data for the LaTeX tables\n",
    "table_data = []\n",
    "\n",
    "for model, (baseline, label) in models_of_interest.items():\n",
    "    accuracies = results_dict[model]\n",
    "    baseline_accuracies = results_dict[baseline]\n",
    "    \n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    \n",
    "    nlu_deltas = {dataset: accuracies[dataset] - baseline_accuracies.get(dataset, 0) for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]}\n",
    "    avg_delta_nlu = sum(nlu_deltas[dataset] for dataset in nlu_deltas) / 4\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in nlu_deltas) / 4\n",
    "    \n",
    "    delta_nlg = accuracies[\"lambada_standard\"] - baseline_accuracies.get(\"lambada_standard\", 0)\n",
    "    avg_acc_nlg = accuracies[\"lambada_standard\"]\n",
    "    \n",
    "    table_data.append((label, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg))\n",
    "\n",
    "# Sort the data by Avg. VL Accuracy and highest NLG Delta\n",
    "table_data = sorted(table_data, key=lambda x: (x[2], -x[5]), reverse=True)\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance}}\n",
    "  \\\\label{tab:model_performance}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|c|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\textbf{VL Avg.} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NLG Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for label, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg in table_data:\n",
    "    latex_code += \"{label} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & {delta_nlu} & {avg_acc_nlu} & {delta_nlg} & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "        label=label,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        avg_acc_vl=format_value(avg_acc_vl),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        avg_acc_nlu=format_value(avg_acc_nlu),\n",
    "        delta_nlg=format_value(delta_nlg),\n",
    "        avg_acc_nlg=format_value(avg_acc_nlg)\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LLaVA + Pythia (1.4B): Avg. VL = 43.97, Avg. NL = 45.51, Delta NL = 2.18, Delta NLU = 0.55, Delta NLG = 8.07\n",
      "Model LLaVA + Pythia Instruct (1.4B): Avg. VL = 43.93, Avg. NL = 41.37, Delta NL = -1.16, Delta NLU = -1.20, Delta NLG = -1.01\n",
      "Model LLaVA + LLaMA2 Instruct (7B): Avg. VL = 56.55, Avg. NL = 64.44, Delta NL = -0.36, Delta NLU = -0.98, Delta NLG = 2.04\n",
      "Model LLaVA + LLaMA2 Base (7B): Avg. VL = 57.22, Avg. NL = 66.23, Delta NL = -1.84, Delta NLU = -2.15, Delta NLG = -0.43\n",
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance}}\n",
      "  \\label{tab:model_performance}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|c|c|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\textbf{Instr.} & \\multicolumn{1}{c|}{\\textbf{VL Avg.}} & \\multicolumn{2}{c}{\\textbf{NL Avg.}} \\\\\n",
      "     & & \\textbf{Acc $\\uparrow$} & \\textbf{Acc $\\uparrow$} & \\textbf{Delta $\\uparrow$} \\\\\n",
      "     \\midrule\n",
      "LLaVA + LLaMA2 Base (7B) & \\ding{55} & 57.22 & 66.23 & -1.84 \\\\\n",
      "LLaVA + LLaMA2 Instruct (7B) & \\ding{51} & 56.55 & 64.44 & -0.36 \\\\\n",
      "LLaVA + Pythia (1.4B) & \\ding{55} & 43.97 & 45.51 & 2.18 \\\\\n",
      "LLaVA + Pythia Instruct (1.4B) & \\ding{51} & 43.93 & 41.37 & -1.16 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import hmean\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the results from the JSON file\n",
    "with open('results_nlp.json') as f:\n",
    "    results_dict = json.load(f)\n",
    "\n",
    "# Define the models of interest and their corresponding baselines with labels\n",
    "models_of_interest = {\n",
    "    \"stage-final-llava-v15-pythia+1p4b\": (\"reproduction-align-pythia+1p4b\", \"LLaVA + Pythia (1.4B)\"),\n",
    "    \"stage-final-llava-v15-pythia+1p4b-instruct\": (\"reproduction-align-pythia+1p4b-instruct\", \"LLaVA + Pythia Instruct (1.4B)\"),\n",
    "    \"reproduction-llava-v15+7b+stage-finetune+x7\": (\"reproduction-llava-v15+7b+stage-align+x7\", \"LLaVA + LLaMA2 Instruct (7B)\"),\n",
    "    \"reproduction-llama2\": (\"vila_base_llm\", \"LLaVA + LLaMA2 Base (7B)\")\n",
    "}\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.2f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Function to check if a model is instruction fine-tuned\n",
    "def is_instruction_fine_tuned(label):\n",
    "    return \"Instruct\" in label\n",
    "\n",
    "# Prepare the data for the LaTeX tables\n",
    "table_data = []\n",
    "\n",
    "for model, (baseline, label) in models_of_interest.items():\n",
    "    accuracies = results_dict[model]\n",
    "    baseline_accuracies = results_dict[baseline]\n",
    "    \n",
    "    avg_acc_vl = hmean([accuracies[dataset] for dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]])\n",
    "    avg_acc_nl = hmean([accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]])\n",
    "    avg_acc_nlu = hmean([accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]])\n",
    "    avg_acc_nlg = accuracies[\"lambada_standard\"]\n",
    "    baseline_avg_nlu = hmean([baseline_accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]])\n",
    "    baseline_avg_nlg = baseline_accuracies[\"lambada_standard\"]\n",
    "    baseline_avg_acc_nl = hmean([baseline_accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]])\n",
    "    delta_nl = baseline_avg_acc_nl - avg_acc_nl\n",
    "    delta_nlu = baseline_avg_nlu - avg_acc_nlu\n",
    "    delta_nlg = baseline_avg_nlg - avg_acc_nlg\n",
    "    print(f\"Model {label}: Avg. VL = {avg_acc_vl*100:.2f}, Avg. NL = {avg_acc_nl*100:.2f}, Delta NL = {delta_nl*100:.2f}, Delta NLU = {delta_nlu*100:.2f}, Delta NLG = {delta_nlg*100:.2f}\")\n",
    "    \n",
    "    table_data.append((label, is_instruction_fine_tuned(label), avg_acc_vl, avg_acc_nl, delta_nl))\n",
    "\n",
    "# Sort the data by Avg. VL Accuracy and highest NL Delta\n",
    "table_data = sorted(table_data, key=lambda x: (x[2], -x[4]), reverse=True)\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance}}\n",
    "  \\\\label{tab:model_performance}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|c|c|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\textbf{Instr.} & \\\\multicolumn{1}{c|}{\\\\textbf{VL Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NL Avg.}} \\\\\\\\\n",
    "     & & \\\\textbf{Acc $\\\\uparrow$} & \\\\textbf{Acc $\\\\uparrow$} & \\\\textbf{Delta $\\\\uparrow$} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for label, instr, avg_acc_vl, avg_acc_nl, delta_nl in table_data:\n",
    "    latex_code += \"{label} & {instr} & {avg_acc_vl} & {avg_acc_nl} & {delta_nl} \\\\\\\\\\n\".format(\n",
    "        label=label,\n",
    "        instr=\"\\\\ding{51}\" if instr else \"\\\\ding{55}\",\n",
    "        avg_acc_vl=format_value(avg_acc_vl),\n",
    "        avg_acc_nl=format_value(avg_acc_nl),\n",
    "        delta_nl=format_value(delta_nl)\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at openai/clip-vit-large-patch14-336 and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.cls_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/localdisk/ssrivas9/miniconda3/envs/prism/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a ViT-L/14@336px model\n",
    "import torch\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "# Load the ViT model. It needs to be the ViT-L/14@336px model: clip-vit-l-336px\n",
    "model = ViTForImageClassification.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
