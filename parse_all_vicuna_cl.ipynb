{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: reproduction-llava-v15+7b+stage-align+x7+bluehive\n",
      "Processing model: lora-stage-0-after-llava-vqav2-peft-policy\n",
      "Processing model: osgm-stage-1-after-llava-ocr\n",
      "Processing model: rehearsal+1-stage-1-after-llava-ocr\n",
      "Processing model: stage-0-after-llava-vqav2\n",
      "Processing model: lora-stage-0-after-llava-vqav2-lora-rank-32\n",
      "Processing model: rehearsal+p1-stage-1-after-llava-ocr\n",
      "Processing model: lora-stage-0-after-llava-vqav2\n",
      "Processing model: soft-stage-0-after-llava-vqav2\n",
      "Processing model: soft-stage-2-after-llava-ref\n",
      "Processing model: sgm-stage-2-after-llava-ref\n",
      "Processing model: qlora-stage-0-after-llava-vqav2-8bit\n",
      "Processing model: olf-stage-0-after-llava-vqav2\n",
      "Processing model: rehearsal+1-stage-2-after-llava-ref\n",
      "Processing model: msgm-stage-0-after-llava-vqav2-8bit\n",
      "Processing model: lora-stage-1-after-llava-ocr\n",
      "Processing model: rehearsal+p1-stage-2-after-llava-ref\n",
      "Processing model: soft-stage-1-after-llava-ocr\n",
      "Processing model: msgm-stage-0-after-llava-vqav2\n",
      "Processing model: reproduction-llava-v15+7b+stage-align+x7\n",
      "Processing model: r1sgm-stage-1-after-llava-ocr\n",
      "Processing model: sgm-stage-0-after-llava-vqav2-soft-alpha-0.1\n",
      "Processing model: stage-2-after-llava-ref\n",
      "Processing model: soft-stage-0-after-llava-vqav2-soft-alpha-0.1\n",
      "Processing model: osgm-stage-0-after-llava-vqav2\n",
      "Processing model: ia3-stage-0-after-llava-vqav2\n",
      "Processing model: qlora-stage-0-after-llava-vqav2\n",
      "Processing model: ptune-stage-0-after-llava-vqav2\n",
      "Processing model: prompt-stage-0-after-llava-vqav2\n",
      "Processing model: r1sgm-stage-2-after-llava-ref\n",
      "Processing model: sgm-stage-0-after-llava-vqav2\n",
      "Processing model: lora-stage-2-after-llava-ref\n",
      "Processing model: stage-1-after-llava-ocr\n",
      "Processing model: lora-stage-0-after-llava-vqav2-lora-rank-128\n",
      "Processing model: osgm-stage-2-after-llava-ref\n",
      "Processing model: sgm-stage-0-after-llava-vqav2-peft-policy\n",
      "Processing model: sgm-stage-1-after-llava-ocr\n",
      "Results saved to results_CL.json\n",
      "Model 'lora' is missing data for at least one run.\n",
      "CL performance change and averages saved to cl_performance_change.json and cl_performance.json respectively.\n",
      "Models missing data for at least one run: ['lora']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_799017/714674061.py:129: RuntimeWarning: Mean of empty slice\n",
      "  avg_delta_vl = np.nanmean([change[dataset] for dataset in baseline_results.keys() if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\", \"refcoco+\", \"refcocog\"]])\n",
      "/tmp/ipykernel_799017/714674061.py:130: RuntimeWarning: Mean of empty slice\n",
      "  avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in baseline_results.keys() if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\", \"refcoco+\", \"refcocog\"]])\n",
      "/tmp/ipykernel_799017/714674061.py:131: RuntimeWarning: Mean of empty slice\n",
      "  avg_delta_nlu = np.nanmean([change[dataset] for dataset in baseline_results.keys() if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"mmlu\"]])\n",
      "/tmp/ipykernel_799017/714674061.py:132: RuntimeWarning: Mean of empty slice\n",
      "  avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in baseline_results.keys() if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"mmlu\"]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the root directory and the datasets of interest\n",
    "root_dir = 'evaluations'\n",
    "datasets = {\n",
    "    'vqa-v2': 'vqa-v2-slim',\n",
    "    'text-vqa': 'text-vqa-slim',\n",
    "    'gqa': 'gqa-slim',\n",
    "    'refcoco': 'refcoco-slim'\n",
    "}\n",
    "nlu_datasets = [\"wsc273\", \"winogrande\", \"lambada_standard\", \"arc_easy\", \"arc_challenge\"]\n",
    "\n",
    "# Initialize the result dictionary\n",
    "result = {}\n",
    "\n",
    "# Iterate through each model folder under the root directory\n",
    "for model_name in os.listdir(root_dir):\n",
    "    model_path = os.path.join(root_dir, model_name)\n",
    "    if os.path.isdir(model_path):\n",
    "        print(f\"Processing model: {model_name}\")\n",
    "        model_result = {}\n",
    "        \n",
    "        # Iterate through each dataset folder under the model folder\n",
    "        for dataset, dataset_slim in datasets.items():\n",
    "            dataset_path = os.path.join(model_path, dataset, dataset_slim, 'prism-clip+7b', 'metrics.json')\n",
    "            if os.path.isfile(dataset_path):\n",
    "                # Parse the metrics.json file\n",
    "                with open(dataset_path, 'r') as f:\n",
    "                    metrics = json.load(f)\n",
    "                    summary = metrics.get('summary', {})\n",
    "                    \n",
    "                    # Capture accuracies based on the dataset\n",
    "                    if dataset == 'vqa-v2':\n",
    "                        accuracy = summary.get('accuracy')\n",
    "                        if accuracy is not None:\n",
    "                            model_result['vqa-v2'] = accuracy / 100.0\n",
    "                    elif dataset == 'text-vqa':\n",
    "                        ocr_accuracy = summary.get('accuracy__TextVQA-OCR')\n",
    "                        pure_accuracy = summary.get('accuracy__TextVQA-Pure')\n",
    "                        if ocr_accuracy is not None:\n",
    "                            model_result['textvqa-ocr'] = ocr_accuracy\n",
    "                        if pure_accuracy is not None:\n",
    "                            model_result['textvqa-pure'] = pure_accuracy\n",
    "                    elif dataset == 'gqa':\n",
    "                        accuracy = summary.get('accuracy')\n",
    "                        if accuracy is not None:\n",
    "                            model_result['gqa'] = accuracy / 100.0\n",
    "                    elif dataset == 'refcoco':\n",
    "                        refcoco_accuracy = summary.get('accuracy__RefCOCO')\n",
    "                        refcocop_accuracy = summary.get('accuracy__RefCOCO+')\n",
    "                        refcocog_accuracy = summary.get('accuracy__RefCOCOg')\n",
    "                        if refcoco_accuracy is not None:\n",
    "                            model_result['refcoco'] = refcoco_accuracy\n",
    "                        if refcocop_accuracy is not None:\n",
    "                            model_result['refcoco+'] = refcocop_accuracy\n",
    "                        if refcocog_accuracy is not None:\n",
    "                            model_result['refcocog'] = refcocog_accuracy\n",
    "\n",
    "        # Parse NLU/NLG results\n",
    "        nlu_path = os.path.join(model_path, 'nlp/nlu', 'results.json')\n",
    "        if os.path.isfile(nlu_path):\n",
    "            with open(nlu_path, 'r') as f:\n",
    "                nlu_results = json.load(f).get('results', {})\n",
    "                for nlu_dataset in nlu_datasets:\n",
    "                    if nlu_dataset in nlu_results:\n",
    "                        acc = nlu_results[nlu_dataset].get('acc,none')\n",
    "                        if acc is not None:\n",
    "                            model_result[nlu_dataset] = acc\n",
    "\n",
    "        # Parse MMLU results\n",
    "        mmlu_path = os.path.join(model_path, 'nlp/mmlu', 'results.json')\n",
    "        if os.path.isfile(mmlu_path):\n",
    "            with open(mmlu_path, 'r') as f:\n",
    "                mmlu_results = json.load(f).get('results', {})\n",
    "                mmlu_acc = mmlu_results.get('mmlu', {}).get('acc,none')\n",
    "                if mmlu_acc is not None:\n",
    "                    model_result['mmlu'] = mmlu_acc\n",
    "\n",
    "        # Add the model results to the final result dictionary\n",
    "        if model_result:\n",
    "            result[model_name] = model_result\n",
    "\n",
    "# Save the final JSON to results_CL.json\n",
    "with open('results_CL.json', 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to results_CL.json\")\n",
    "\n",
    "# Now process the CL runs\n",
    "cl_runs = {\n",
    "    \"sgm\": [\"sgm-stage-0-after-llava-vqav2-peft-policy\", \"sgm-stage-1-after-llava-ocr\", \"sgm-stage-2-after-llava-ref\"],\n",
    "    \"r1_sgm\": [\"sgm-stage-0-after-llava-vqav2-peft-policy\", \"r1sgm-stage-1-after-llava-ocr\", \"r1sgm-stage-2-after-llava-ref\"],\n",
    "    \"o_sgm\": [\"osgm-stage-0-after-llava-vqav2\", \"osgm-stage-1-after-llava-ocr\", \"osgm-stage-2-after-llava-ref\"],\n",
    "    \"rehearsal_p1\": [\"stage-0-after-llava-vqav2\", \"rehearsal+p1-stage-1-after-llava-ocr\", \"rehearsal+p1-stage-2-after-llava-ref\"],\n",
    "    \"rehearsal_1\": [\"stage-0-after-llava-vqav2\", \"rehearsal+1-stage-1-after-llava-ocr\", \"rehearsal+1-stage-2-after-llava-ref\"],\n",
    "    \"lora\": [\"lora-stage-0-after-llava-vqav2\", \"lora-stage-1-after-llava-ocr\", \"lora-stage-2-after-llava-ref\"],\n",
    "    \"naive-ft\": [\"stage-0-after-llava-vqav2\", \"stage-1-after-llava-ocr\", \"stage-2-after-llava-ref\"],\n",
    "    \"soft\": [\"soft-stage-0-after-llava-vqav2\", \"soft-stage-1-after-llava-ocr\", \"soft-stage-2-after-llava-ref\"]\n",
    "}\n",
    "\n",
    "baseline_run_id = 'reproduction-llava-v15+7b+stage-align+x7'\n",
    "baseline_results = result[baseline_run_id]\n",
    "\n",
    "# Calculate performance changes and averages for each CL run\n",
    "cl_performance_change = {}\n",
    "cl_performance = {}\n",
    "models_missing_data = []\n",
    "\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    changes = {}\n",
    "    performances = {}\n",
    "    missing_data = False\n",
    "\n",
    "    for i, run_id in enumerate(run_ids):\n",
    "        if run_id not in result:\n",
    "            missing_data = True\n",
    "            break\n",
    "        current_results = result[run_id]\n",
    "        if 'refcoco' not in current_results or 'refcoco+' not in current_results or 'refcocog' not in current_results:\n",
    "            missing_data = True\n",
    "            break\n",
    "        \n",
    "        change = {dataset: current_results.get(dataset, np.nan) - baseline_results.get(dataset, np.nan) for dataset in baseline_results.keys()}\n",
    "        changes[f'task_{i+1}'] = change\n",
    "        performances[f'task_{i+1}'] = current_results\n",
    "        \n",
    "        avg_delta_vl = np.nanmean([change[dataset] for dataset in baseline_results.keys() if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\", \"refcoco+\", \"refcocog\"]])\n",
    "        avg_acc_vl = np.nanmean([current_results.get(dataset, np.nan) for dataset in baseline_results.keys() if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\", \"refcoco\", \"refcoco+\", \"refcocog\"]])\n",
    "        avg_delta_nlu = np.nanmean([change[dataset] for dataset in baseline_results.keys() if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"mmlu\"]])\n",
    "        avg_acc_nlu = np.nanmean([current_results.get(dataset, np.nan) for dataset in baseline_results.keys() if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"mmlu\"]])\n",
    "        \n",
    "        changes[f'task_{i+1}_avg'] = {'VL': avg_delta_vl, 'NLU/NLG': avg_delta_nlu}\n",
    "        performances[f'task_{i+1}_avg'] = {'VL': avg_acc_vl, 'NLU/NLG': avg_acc_nlu}\n",
    "    \n",
    "    if missing_data:\n",
    "        models_missing_data.append(model_name)\n",
    "        print(f\"Model '{model_name}' is missing data for at least one run.\")\n",
    "    else:\n",
    "        cl_performance_change[model_name] = changes\n",
    "        cl_performance[model_name] = performances\n",
    "\n",
    "# Save the performance changes and averages to JSON files\n",
    "with open('cl_performance_change.json', 'w') as f:\n",
    "    json.dump(cl_performance_change, f, indent=2)\n",
    "\n",
    "with open('cl_performance.json', 'w') as f:\n",
    "    json.dump(cl_performance, f, indent=2)\n",
    "\n",
    "print(\"CL performance change and averages saved to cl_performance_change.json and cl_performance.json respectively.\")\n",
    "\n",
    "if models_missing_data:\n",
    "    print(f\"Models missing data for at least one run: {models_missing_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cl_runs = {\n",
    "    \"sgm\": [\"sgm-stage-0-after-llava-vqav2-peft-policy\", \"sgm-stage-1-after-llava-ocr\", \"sgm-stage-2-after-llava-ref\"],\n",
    "    \"r1_sgm\": [\"sgm-stage-0-after-llava-vqav2-peft-policy\", \"r1sgm-stage-1-after-llava-ocr\", \"r1sgm-stage-2-after-llava-ref\"],\n",
    "    \"o_sgm\": [\"osgm-stage-0-after-llava-vqav2\", \"osgm-stage-1-after-llava-ocr\", \"osgm-stage-2-after-llava-ref\"],\n",
    "    \"rehearsal_p1\": [\"stage-0-after-llava-vqav2\", \"rehearsal+p1-stage-1-after-llava-ocr\", \"rehearsal+p1-stage-2-after-llava-ref\"],\n",
    "    \"rehearsal_1\": [\"stage-0-after-llava-vqav2\", \"rehearsal+1-stage-1-after-llava-ocr\", \"rehearsal+1-stage-2-after-llava-ref\"],\n",
    "    \"lora\": [\"lora-stage-0-after-llava-vqav2\", \"lora-stage-1-after-llava-ocr\", \"lora-stage-2-after-llava-ref\"],\n",
    "    \"naive-ft\": [\"stage-0-after-llava-vqav2\", \"stage-1-after-llava-ocr\", \"stage-2-after-llava-ref\"],\n",
    "    \"soft\": [\"soft-stage-0-after-llava-vqav2\", \"soft-stage-1-after-llava-ocr\", \"soft-stage-2-after-llava-ref\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models without refcoco dataset:\n",
      "soft-stage-0-after-llava-vqav2\n",
      "rehearsal+p1-stage-1-after-llava-ocr\n",
      "sgm-stage-0-after-llava-vqav2-peft-policy\n",
      "osgm-stage-1-after-llava-ocr\n",
      "stage-0-after-llava-vqav2\n",
      "osgm-stage-2-after-llava-ref\n",
      "osgm-stage-0-after-llava-vqav2\n",
      "\n",
      "Models without mmlu dataset:\n",
      "osgm-stage-1-after-llava-ocr\n",
      "osgm-stage-2-after-llava-ref\n",
      "osgm-stage-0-after-llava-vqav2\n",
      "\n",
      "Models without both refcoco and mmlu datasets:\n",
      "osgm-stage-1-after-llava-ocr\n",
      "osgm-stage-2-after-llava-ref\n",
      "osgm-stage-0-after-llava-vqav2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the results_CL.json file\n",
    "results_file = 'results_CL.json'\n",
    "with open(results_file, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Define the CL runs\n",
    "cl_runs = {\n",
    "    \"sgm\": [\"sgm-stage-0-after-llava-vqav2-peft-policy\", \"sgm-stage-1-after-llava-ocr\", \"sgm-stage-2-after-llava-ref\"],\n",
    "    \"r1_sgm\": [\"sgm-stage-0-after-llava-vqav2-peft-policy\", \"r1sgm-stage-1-after-llava-ocr\", \"r1sgm-stage-2-after-llava-ref\"],\n",
    "    \"o_sgm\": [\"osgm-stage-0-after-llava-vqav2\", \"osgm-stage-1-after-llava-ocr\", \"osgm-stage-2-after-llava-ref\"],\n",
    "    \"rehearsal_p1\": [\"stage-0-after-llava-vqav2\", \"rehearsal+p1-stage-1-after-llava-ocr\", \"rehearsal+p1-stage-2-after-llava-ref\"],\n",
    "    \"rehearsal_1\": [\"stage-0-after-llava-vqav2\", \"rehearsal+1-stage-1-after-llava-ocr\", \"rehearsal+1-stage-2-after-llava-ref\"],\n",
    "    \"lora\": [\"lora-stage-0-after-llava-vqav2\", \"lora-stage-1-after-llava-ocr\", \"lora-stage-2-after-llava-ref\"],\n",
    "    \"naive-ft\": [\"stage-0-after-llava-vqav2\", \"stage-1-after-llava-ocr\", \"stage-2-after-llava-ref\"],\n",
    "    \"soft\": [\"soft-stage-0-after-llava-vqav2\", \"soft-stage-1-after-llava-ocr\", \"soft-stage-2-after-llava-ref\"]\n",
    "}\n",
    "\n",
    "# Lists to store model names without refcoco and mmlu datasets\n",
    "models_without_refcoco = []\n",
    "models_without_mmlu = []\n",
    "models_without_both = []\n",
    "\n",
    "# Check each model in cl_runs for refcoco and mmlu datasets\n",
    "for model_name, run_ids in cl_runs.items():\n",
    "    has_refcoco = False\n",
    "    has_mmlu = False\n",
    "    \n",
    "    for run_id in run_ids:\n",
    "        if run_id in results:\n",
    "            if 'refcoco' in results[run_id] or 'refcoco+' in results[run_id] or 'refcocog' in results[run_id]:\n",
    "                has_refcoco = True\n",
    "            if 'mmlu' in results[run_id]:\n",
    "                has_mmlu = True\n",
    "            if has_refcoco and has_mmlu:\n",
    "                break\n",
    "                \n",
    "        if not has_refcoco:\n",
    "            models_without_refcoco.append(run_id)\n",
    "        if not has_mmlu:\n",
    "            models_without_mmlu.append(run_id)\n",
    "        if not has_refcoco and not has_mmlu:\n",
    "            models_without_both.append(run_id)\n",
    "\n",
    "models_without_both = set(models_without_both)\n",
    "models_without_mmlu  = set(models_without_mmlu)\n",
    "models_without_refcoco = set(models_without_refcoco)\n",
    "# Print the models without refcoco, mmlu, and both datasets\n",
    "print(\"Models without refcoco dataset:\")\n",
    "for model in models_without_refcoco:\n",
    "    print(model)\n",
    "\n",
    "print(\"\\nModels without mmlu dataset:\")\n",
    "for model in models_without_mmlu:\n",
    "    print(model)\n",
    "\n",
    "print(\"\\nModels without both refcoco and mmlu datasets:\")\n",
    "for model in models_without_both:\n",
    "    print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input file name and load the results data\n",
    "results_file = 'results_A.json'\n",
    "\n",
    "with open(results_file, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Define the list of models and the rules for matching\n",
    "models = ['soft', 'lora', 'qlora', 'msgm', 'sgm', 'ptune', 'prompt', 'ia3', 'naive-ft', 'vicuna', 'olf']\n",
    "datasets = [\n",
    "    'vqa-v2', 'textvqa-ocr', 'textvqa-pure', 'gqa',\n",
    "    'wsc273', 'winogrande', 'lambada_standard', 'arc_easy', 'arc_challenge', 'mmlu'\n",
    "]\n",
    "model_mappings = {model: [] for model in models}\n",
    "\n",
    "# Function to map model_name to category\n",
    "def get_category(model_name):\n",
    "    if 'reproduction' in model_name:\n",
    "        return 'vicuna'\n",
    "    if model_name.startswith('stage'):\n",
    "        return 'naive-ft'\n",
    "    for model in models:\n",
    "        if model in ['msgm', 'sgm']:\n",
    "            if 'msgm' in model_name:\n",
    "                return 'msgm'\n",
    "            if 'sgm' in model_name:\n",
    "                return 'sgm'\n",
    "        if model in model_name and not (model == 'lora' and 'qlora' in model_name):\n",
    "            return model\n",
    "    return None\n",
    "\n",
    "# Populate the mappings based on model name rules\n",
    "for model_name, metrics in results.items():\n",
    "    # Ensure the model has all required datasets\n",
    "    if all(dataset in metrics for dataset in datasets):\n",
    "        category = get_category(model_name)\n",
    "        if category:\n",
    "            accuracies = list(metrics.values())\n",
    "            avg_accuracy = np.nanmean(accuracies)\n",
    "            model_mappings[category].append((model_name, avg_accuracy))\n",
    "        else:\n",
    "            for model in models:\n",
    "                if model in model_name and not (model == 'lora' and 'qlora' in model_name):\n",
    "                    model_mappings[model].append((model_name, np.nanmean(list(metrics.values()))))\n",
    "    else:\n",
    "        missing_datasets = [dataset for dataset in datasets if dataset not in metrics]\n",
    "        print(f\"Warning: Model '{model_name}' is missing datasets: {missing_datasets}\")\n",
    "\n",
    "# Identify the highest accuracy model for each mapping\n",
    "highest_accuracy_models = {}\n",
    "for model, mappings in model_mappings.items():\n",
    "    if mappings:\n",
    "        highest_accuracy_models[model] = max(mappings, key=lambda x: x[1])\n",
    "\n",
    "# Save the highest accuracy models to a file\n",
    "output_file = 'highest_accuracy_models.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(highest_accuracy_models, f, indent=2)\n",
    "\n",
    "# Map the highest accuracy models to the new names\n",
    "name_mapping = {\n",
    "    'soft': 'Soft Targets',\n",
    "    'msgm': 'mSGM',\n",
    "    'sgm': 'SGM',\n",
    "    'lora': 'LoRA',\n",
    "    'qlora': 'QLoRA',\n",
    "    'ia3': 'IA3',\n",
    "    'prompt': 'Prompt Tuning',\n",
    "    'ptune': 'P-Tuning',\n",
    "    'naive-ft': 'LLaVA Naive-FT',\n",
    "    'vicuna': 'Vicuna 1.5 7B',\n",
    "    'olf': 'Output Layer Freezing'\n",
    "}\n",
    "\n",
    "# Prepare data for radar chart\n",
    "methods = [name_mapping[model] for model in models if model in highest_accuracy_models]\n",
    "results_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    if model in highest_accuracy_models:\n",
    "        model_name, _ = highest_accuracy_models[model]\n",
    "        metrics = results[model_name]\n",
    "        accuracies = {dataset: metrics.get(dataset, np.nan) for dataset in datasets}\n",
    "        results_dict[name_mapping[model]] = accuracies\n",
    "\n",
    "# Prepare data for radar chart\n",
    "vl_datasets = ['vqa-v2', 'textvqa-ocr', 'textvqa-pure', 'gqa']\n",
    "nlu_datasets = ['wsc273', 'winogrande', 'arc_easy', 'arc_challenge', 'mmlu']\n",
    "nlg_datasets = ['lambada_standard']\n",
    "datasets = vl_datasets + nlu_datasets + nlg_datasets\n",
    "\n",
    "methods = list(results_dict.keys())\n",
    "results = []\n",
    "\n",
    "for method in methods:\n",
    "    accuracies = [results_dict[method].get(dataset, np.nan) for dataset in datasets]\n",
    "    results.append(accuracies)\n",
    "\n",
    "# Convert to numpy array\n",
    "results = np.array(results)\n",
    "\n",
    "# Normalize the values for the plot\n",
    "def normalize_values(values, min_val=0.2, max_val=1.0):\n",
    "    norm_values = np.zeros_like(values)\n",
    "    for i in range(values.shape[1]):\n",
    "        col_min = np.nanmin(values[:, i])\n",
    "        col_max = np.nanmax(values[:, i])\n",
    "        norm_values[:, i] = (values[:, i] - col_min) / (col_max - col_min) * (max_val - min_val) + min_val\n",
    "    return norm_values\n",
    "\n",
    "plot_results = normalize_values(results)\n",
    "\n",
    "# Close the loop for the radar chart\n",
    "methods.append(methods[0])\n",
    "plot_results = np.concatenate((plot_results, plot_results[:, [0]]), axis=1)\n",
    "results = np.concatenate((results, results[:, [0]]), axis=1)\n",
    "\n",
    "# Custom labels with line breaks\n",
    "labels = ['VQAv2\\n(Vision Language)', 'TextVQA OCR\\n(Vision Language)', 'TextVQA Pure\\n(Vision Language)', 'GQA\\n(Vision Language)',\n",
    "          'WSC273\\n(NLU)', 'Winogrande\\n(NLU)', 'ARC Easy\\n(NLU)', 'ARC Challenge\\n(NLU)', 'Lambada\\n(NLG)', 'MMLU\\n(NLU)', 'VQAv2\\n(Vision Language)']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16), subplot_kw=dict(polar=True))\n",
    "label_loc = np.linspace(start=0, stop=2 * np.pi, num=len(labels))\n",
    "\n",
    "# Radar lines\n",
    "linestyles = [\"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\", \"solid\"]\n",
    "markers = [\"o\", \"^\", \"d\", \"s\", \"p\", \"*\", \"h\", \"x\", \"+\", \"v\", \"P\"]\n",
    "colors = [\"#FFD700\", \"#FFA500\", \"#FF4500\", \"#32CD32\", \"#1E90FF\", \"#FF69B4\", \"#8A2BE2\", \"#A52A2A\", \"#20B2AA\", \"#FF6347\", \"#2E8B57\"]\n",
    "\n",
    "for res_ix, res in enumerate(plot_results):\n",
    "    ax.plot(label_loc, res, lw=6, ls=linestyles[res_ix], marker=markers[res_ix], markersize=14, label=methods[res_ix], color=colors[res_ix])\n",
    "\n",
    "# VQAv2 at the top, remaining metrics are added in a clockwise direction\n",
    "ax.set_theta_zero_location('N')\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Concentric circles\n",
    "thickness = 0.3\n",
    "colors_circles = [\"#D1EDD2\", \"#A8CACE\", \"#83C9AD\", \"#56A3F1\"]\n",
    "ax.set_rticks(np.linspace(thickness, 1.0, len(colors_circles)))\n",
    "ax.set_yticklabels([])\n",
    "for ix in range(len(colors_circles), 0, -1):\n",
    "    circ = plt.Circle((0, 0), ix * thickness, transform=ax.transData._b, color=colors_circles[ix - 1])\n",
    "    ax.add_artist(circ)\n",
    "\n",
    "# Increase font size for readability and spacing of labels\n",
    "plt.rc('font', family='serif', size=28)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=36)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=32)     # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=28)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=28)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=28)    # legend fontsize\n",
    "plt.rc('figure', titlesize=40)   # fontsize of the figure title\n",
    "\n",
    "# Apply the larger font size to the labels and space them out\n",
    "lines, labels = plt.thetagrids(np.degrees(label_loc), labels=labels, fontsize=28, verticalalignment='top')\n",
    "ax.tick_params(pad=100)  # Increase padding to space the labels further from the chart\n",
    "\n",
    "# Annotate the axes with the true accuracy values\n",
    "for i, label in enumerate(labels[:-1]):  # Exclude the last label which is a duplicate\n",
    "    angle_rad = label_loc[i]\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    ha = 'center'\n",
    "    if angle_deg > 90 and angle_deg < 270:\n",
    "        angle_deg = angle_deg + 180\n",
    "        ha = 'center'\n",
    "    ax.text(angle_rad, 1.07, f\"{np.nanmax(results[:, i]):.3f}\", size=24, horizontalalignment=ha, verticalalignment='center')\n",
    "\n",
    "# Adding legend at the top, horizontal, and spaced out\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=2, frameon=False, handletextpad=3, columnspacing=10)\n",
    "\n",
    "# Save the plot as SVG\n",
    "plt.savefig('radar_chart.svg', format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "naive_ft_acc = results_dict[\"LLaVA Naive-FT\"]\n",
    "vicuna_acc = results_dict.get(\"Vicuna 1.5 7B\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model in [\"LLaVA Naive-FT\", \"Vicuna 1.5 7B\"]:\n",
    "        continue\n",
    "    deltas = {dataset: naive_ft_acc[dataset] - acc for dataset, acc in accuracies.items()}\n",
    "    avg_delta_vl = sum(deltas[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    avg_delta_nlu = sum(deltas[dataset] for dataset in accuracies if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"mmlu\"]) / 5\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"mmlu\"]) / 5\n",
    "    table_data.append((model, accuracies, deltas, avg_delta_vl, avg_acc_vl, avg_delta_nlu, avg_acc_nlu))\n",
    "\n",
    "# Sort the data so models with smallest average delta VL are first\n",
    "table_data = sorted(table_data, key=lambda x: x[3])\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Task-wise Accuracies of Each Mitigation Method across Vision-Language (VL) and NLU tasks}\n",
    "  \\\\label{tab:vl_nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|cc|ccccc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\multicolumn{2}{c|}{\\\\textbf{VL Avg.}} & \\\\multicolumn{5}{c|}{\\\\textbf{NLU}} & \\\\multicolumn{2}{c}{\\\\textbf{NLU Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & $\\\\Delta$ & Acc & \\\\textbf{WSC273} & \\\\textbf{Winogrande} & \\\\textbf{ARC Easy} & \\\\textbf{ARC Challenge} & \\\\textbf{MMLU} & $\\\\Delta$ & Acc \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "latex_code += \"LLaVA Naive-FT & {vqa_v2:.3f} & {textvqa_ocr:.3f} & {textvqa_pure:.3f} & {gqa:.3f} & - & {avg_acc_vl:.3f} & {wsc273:.3f} & {winogrande:.3f} & {arc_easy:.3f} & {arc_challenge:.3f} & {mmlu:.3f} & - & {avg_acc_nlu:.3f} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=naive_ft_acc[\"vqa-v2\"],\n",
    "    textvqa_ocr=naive_ft_acc[\"textvqa-ocr\"],\n",
    "    textvqa_pure=naive_ft_acc[\"textvqa-pure\"],\n",
    "    gqa=naive_ft_acc[\"gqa\"],\n",
    "    avg_acc_vl=(naive_ft_acc[\"vqa-v2\"] + naive_ft_acc[\"textvqa-ocr\"] + naive_ft_acc[\"textvqa-pure\"] + naive_ft_acc[\"gqa\"]) / 4,\n",
    "    wsc273=naive_ft_acc[\"wsc273\"],\n",
    "    winogrande=naive_ft_acc[\"winogrande\"],\n",
    "    arc_easy=naive_ft_acc[\"arc_easy\"],\n",
    "    arc_challenge=naive_ft_acc[\"arc_challenge\"],\n",
    "    mmlu=naive_ft_acc.get(\"mmlu\", np.nan),\n",
    "    avg_acc_nlu=(naive_ft_acc[\"wsc273\"] + naive_ft_acc[\"winogrande\"] + naive_ft_acc[\"arc_easy\"] + naive_ft_acc[\"arc_challenge\"] + naive_ft_acc.get(\"mmlu\", np.nan)) / 5\n",
    ")\n",
    "latex_code += \"Vicuna 1.5 7B & {vqa_v2:.3f} & {textvqa_ocr:.3f} & {textvqa_pure:.3f} & {gqa:.3f} & - & {avg_acc_vl:.3f} & {wsc273:.3f} & {winogrande:.3f} & {arc_easy:.3f} & {arc_challenge:.3f} & {mmlu:.3f} & - & {avg_acc_nlu:.3f} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=vicuna_acc.get(\"vqa-v2\", np.nan),\n",
    "    textvqa_ocr=vicuna_acc.get(\"textvqa-ocr\", np.nan),\n",
    "    textvqa_pure=vicuna_acc.get(\"textvqa-pure\", np.nan),\n",
    "    gqa=vicuna_acc.get(\"gqa\", np.nan),\n",
    "    avg_acc_vl=(vicuna_acc.get(\"vqa-v2\", 0) + vicuna_acc.get(\"textvqa-ocr\", 0) + vicuna_acc.get(\"textvqa-pure\", 0) + vicuna_acc.get(\"gqa\", 0)) / 4,\n",
    "    wsc273=vicuna_acc.get(\"wsc273\", np.nan),\n",
    "    winogrande=vicuna_acc.get(\"winogrande\", np.nan),\n",
    "    arc_easy=vicuna_acc.get(\"arc_easy\", np.nan),\n",
    "    arc_challenge=vicuna_acc.get(\"arc_challenge\", np.nan),\n",
    "    mmlu=vicuna_acc.get(\"mmlu\", np.nan),\n",
    "    avg_acc_nlu=(vicuna_acc.get(\"wsc273\", 0) + vicuna_acc.get(\"winogrande\", 0) + vicuna_acc.get(\"arc_easy\", 0) + vicuna_acc.get(\"arc_challenge\", 0) + vicuna_acc.get(\"mmlu\", 0)) / 5\n",
    ")\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, deltas, avg_delta_vl, avg_acc_vl, avg_delta_nlu, avg_acc_nlu in table_data:\n",
    "    latex_code += \"{model} & {vqa_v2:.3f} & {textvqa_ocr:.3f} & {textvqa_pure:.3f} & {gqa:.3f} & {delta_vl:.3f} & {acc_vl:.3f} & {wsc273:.3f} & {winogrande:.3f} & {arc_easy:.3f} & {arc_challenge:.3f} & {mmlu:.3f} & {delta_nlu:.3f} & {acc_nlu:.3f} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=accuracies[\"vqa-v2\"],\n",
    "        textvqa_ocr=accuracies[\"textvqa-ocr\"],\n",
    "        textvqa_pure=accuracies[\"textvqa-pure\"],\n",
    "        gqa=accuracies[\"gqa\"],\n",
    "        delta_vl=avg_delta_vl,\n",
    "        acc_vl=avg_acc_vl,\n",
    "        wsc273=accuracies[\"wsc273\"],\n",
    "        winogrande=accuracies[\"winogrande\"],\n",
    "        arc_easy=accuracies[\"arc_easy\"],\n",
    "        arc_challenge=accuracies[\"arc_challenge\"],\n",
    "        mmlu=accuracies.get(\"mmlu\", np.nan),\n",
    "        delta_nlu=avg_delta_nlu,\n",
    "        acc_nlu=avg_acc_nlu\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
