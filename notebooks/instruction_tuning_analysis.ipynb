{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LLaVA + Pythia (1.4B): Avg. VL = 43.97, Avg. NL = 45.51, Delta NL = 2.18, Delta NLU = 0.55, Delta NLG = 8.07\n",
      "Model LLaVA + Pythia Instruct (1.4B): Avg. VL = 43.93, Avg. NL = 41.37, Delta NL = -1.16, Delta NLU = -1.20, Delta NLG = -1.01\n",
      "Model LLaVA + LLaMA2 Instruct (7B): Avg. VL = 56.55, Avg. NL = 64.44, Delta NL = -0.36, Delta NLU = -0.98, Delta NLG = 2.04\n",
      "Model LLaVA + LLaMA2 Base (7B): Avg. VL = 57.22, Avg. NL = 66.23, Delta NL = -1.84, Delta NLU = -2.15, Delta NLG = -0.43\n",
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance}}\n",
      "  \\label{tab:model_performance}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|c|c|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\textbf{Instr.} & \\multicolumn{1}{c|}{\\textbf{VL Avg.}} & \\multicolumn{2}{c}{\\textbf{NL Avg.}} \\\\\n",
      "     & & \\textbf{Acc $\\uparrow$} & \\textbf{Acc $\\uparrow$} & \\textbf{Delta $\\uparrow$} \\\\\n",
      "     \\midrule\n",
      "LLaVA + LLaMA2 Base (7B) & \\ding{55} & 57.22 & 66.23 & -1.84 \\\\\n",
      "LLaVA + LLaMA2 Instruct (7B) & \\ding{51} & 56.55 & 64.44 & -0.36 \\\\\n",
      "LLaVA + Pythia (1.4B) & \\ding{55} & 43.97 & 45.51 & 2.18 \\\\\n",
      "LLaVA + Pythia Instruct (1.4B) & \\ding{51} & 43.93 & 41.37 & -1.16 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import hmean\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the results from the JSON file\n",
    "with open('results_nlp.json') as f:\n",
    "    results_dict = json.load(f)\n",
    "\n",
    "# Define the models of interest and their corresponding baselines with labels\n",
    "models_of_interest = {\n",
    "    \"stage-final-llava-v15-pythia+1p4b\": (\"reproduction-align-pythia+1p4b\", \"LLaVA + Pythia (1.4B)\"),\n",
    "    \"stage-final-llava-v15-pythia+1p4b-instruct\": (\"reproduction-align-pythia+1p4b-instruct\", \"LLaVA + Pythia Instruct (1.4B)\"),\n",
    "    \"reproduction-llava-v15+7b+stage-finetune+x7\": (\"reproduction-llava-v15+7b+stage-align+x7\", \"LLaVA + LLaMA2 Instruct (7B)\"),\n",
    "    \"reproduction-llama2\": (\"vila_base_llm\", \"LLaVA + LLaMA2 Base (7B)\")\n",
    "}\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.2f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Function to check if a model is instruction fine-tuned\n",
    "def is_instruction_fine_tuned(label):\n",
    "    return \"Instruct\" in label\n",
    "\n",
    "# Prepare the data for the LaTeX tables\n",
    "table_data = []\n",
    "\n",
    "for model, (baseline, label) in models_of_interest.items():\n",
    "    accuracies = results_dict[model]\n",
    "    baseline_accuracies = results_dict[baseline]\n",
    "    \n",
    "    avg_acc_vl = hmean([accuracies[dataset] for dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]])\n",
    "    avg_acc_nl = hmean([accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]])\n",
    "    avg_acc_nlu = hmean([accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]])\n",
    "    avg_acc_nlg = accuracies[\"lambada_standard\"]\n",
    "    baseline_avg_nlu = hmean([baseline_accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]])\n",
    "    baseline_avg_nlg = baseline_accuracies[\"lambada_standard\"]\n",
    "    baseline_avg_acc_nl = hmean([baseline_accuracies[dataset] for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]])\n",
    "    delta_nl = baseline_avg_acc_nl - avg_acc_nl\n",
    "    delta_nlu = baseline_avg_nlu - avg_acc_nlu\n",
    "    delta_nlg = baseline_avg_nlg - avg_acc_nlg\n",
    "    print(f\"Model {label}: Avg. VL = {avg_acc_vl*100:.2f}, Avg. NL = {avg_acc_nl*100:.2f}, Delta NL = {delta_nl*100:.2f}, Delta NLU = {delta_nlu*100:.2f}, Delta NLG = {delta_nlg*100:.2f}\")\n",
    "    \n",
    "    table_data.append((label, is_instruction_fine_tuned(label), avg_acc_vl, avg_acc_nl, delta_nl))\n",
    "\n",
    "# Sort the data by Avg. VL Accuracy and highest NL Delta\n",
    "table_data = sorted(table_data, key=lambda x: (x[2], -x[4]), reverse=True)\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance}}\n",
    "  \\\\label{tab:model_performance}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|c|c|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\textbf{Instr.} & \\\\multicolumn{1}{c|}{\\\\textbf{VL Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NL Avg.}} \\\\\\\\\n",
    "     & & \\\\textbf{Acc $\\\\uparrow$} & \\\\textbf{Acc $\\\\uparrow$} & \\\\textbf{Delta $\\\\uparrow$} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for label, instr, avg_acc_vl, avg_acc_nl, delta_nl in table_data:\n",
    "    latex_code += \"{label} & {instr} & {avg_acc_vl} & {avg_acc_nl} & {delta_nl} \\\\\\\\\\n\".format(\n",
    "        label=label,\n",
    "        instr=\"\\\\ding{51}\" if instr else \"\\\\ding{55}\",\n",
    "        avg_acc_vl=format_value(avg_acc_vl),\n",
    "        avg_acc_nl=format_value(avg_acc_nl),\n",
    "        delta_nl=format_value(delta_nl)\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{Vision‑Language Task Accuracies} -- post‑training on each model.}\n",
      "  \\label{tab:vl_accuracies}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{%\n",
      "    \\begin{tabular}{l|cccc}\n",
      "      \\toprule\n",
      "      \\textbf{Model} & \\textbf{VQA} & \\textbf{TextVQA‑OCR} & \\textbf{TextVQA‑Pure} & \\textbf{GQA} \\\\\n",
      "      \\midrule\n",
      "      LLaVA + Pythia (1.4B) & 66.17 & 38.49 & 35.49 & 46.09 \\\\\n",
      "      LLaVA + Pythia Instruct (1.4B) & 66.46 & 39.12 & 34.35 & 46.88 \\\\\n",
      "      LLaVA + LLaMA2 Instruct (7B) & 74.50 & 56.28 & 45.95 & 56.25 \\\\\n",
      "      LLaVA + LLaMA2 Base (7B) & 75.88 & 55.21 & 45.43 & 60.25 \\\\\n",
      "      \\bottomrule\n",
      "    \\end{tabular}%\n",
      "}\n",
      "\\end{table*}\n",
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{Natural Language Task Accuracies} -- post‑training on each model.}\n",
      "  \\label{tab:nl_accuracies}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{%\n",
      "    \\begin{tabular}{l|ccccc}\n",
      "      \\toprule\n",
      "      \\textbf{Model} & \\textbf{WSC273} & \\textbf{Winogrande} & \\textbf{ARC‑E} & \\textbf{ARC‑C} & \\textbf{Lambada} \\\\\n",
      "      \\midrule\n",
      "      LLaVA + Pythia (1.4B) & 67.40 & 56.20 & 60.98 & 27.47 & 40.91 \\\\\n",
      "      LLaVA + Pythia Instruct (1.4B) & 64.84 & 54.06 & 51.94 & 25.68 & 34.80 \\\\\n",
      "      LLaVA + LLaMA2 Instruct (7B) & 85.35 & 68.35 & 75.97 & 45.39 & 62.31 \\\\\n",
      "      LLaVA + LLaMA2 Base (7B) & 87.91 & 70.56 & 77.06 & 44.62 & 68.70 \\\\\n",
      "      \\bottomrule\n",
      "    \\end{tabular}%\n",
      "}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- INSERT *AFTER* loading results_dict and defining models_of_interest, format_value, is_instruction_fine_tuned ---\n",
    "\n",
    "# Define VL and NL splits and human‑readable labels\n",
    "vl_cols = [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]\n",
    "vl_labels = {\"vqa-v2\": \"VQA\", \"textvqa-ocr\": \"TextVQA‑OCR\", \"textvqa-pure\": \"TextVQA‑Pure\", \"gqa\": \"GQA\"}\n",
    "\n",
    "nl_cols = [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]\n",
    "nl_labels = {\"wsc273\": \"WSC273\", \"winogrande\": \"Winogrande\", \"arc_easy\": \"ARC‑E\", \"arc_challenge\": \"ARC‑C\", \"lambada_standard\": \"Lambada\"}\n",
    "\n",
    "# Function to emit a LaTeX table given a column split\n",
    "def emit_table(split_cols, split_labels, caption, label):\n",
    "    # begin table\n",
    "    print(r\"\\begin{table*}[h]\")\n",
    "    print(r\"  \\caption{\" + caption + r\"}\")\n",
    "    print(r\"  \\label{\" + label + r\"}\")\n",
    "    print(r\"  \\centering\")\n",
    "    print(r\"  \\resizebox{\\linewidth}{!}{%\")\n",
    "    # tabular header: one 'l' for model name + one 'c' per split column\n",
    "    cols_fmt = \"l|\" + \"c\" * len(split_cols)\n",
    "    print(r\"    \\begin{tabular}{\" + cols_fmt + r\"}\")\n",
    "    print(r\"      \\toprule\")\n",
    "    # header row\n",
    "    headers = \" & \".join([r\"\\textbf{\" + split_labels[c] + r\"}\" for c in split_cols])\n",
    "    print(r\"      \\textbf{Model} & \" + headers + r\" \\\\\")\n",
    "    print(r\"      \\midrule\")\n",
    "    # data rows\n",
    "    for model_key, (baseline, label) in models_of_interest.items():\n",
    "        row = [label]\n",
    "        accs = results_dict[model_key]\n",
    "        for c in split_cols:\n",
    "            val = accs.get(c, np.nan)\n",
    "            row.append(format_value(val))\n",
    "        print(\"      \" + \" & \".join(row) + r\" \\\\\")\n",
    "    # end tabular\n",
    "    print(r\"      \\bottomrule\")\n",
    "    print(r\"    \\end{tabular}%\")\n",
    "    print(r\"}\")\n",
    "    print(r\"\\end{table*}\")\n",
    "    print()\n",
    "\n",
    "# Emit VL accuracy table\n",
    "emit_table(\n",
    "    vl_cols,\n",
    "    vl_labels,\n",
    "    caption=r\"\\textbf{Vision‑Language Task Accuracies} -- post‑training on each model.\",\n",
    "    label=\"tab:vl_accuracies\"\n",
    ")\n",
    "\n",
    "# Emit NL accuracy table\n",
    "emit_table(\n",
    "    nl_cols,\n",
    "    nl_labels,\n",
    "    caption=r\"\\textbf{Natural Language Task Accuracies} -- post‑training on each model.\",\n",
    "    label=\"tab:nl_accuracies\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[h]\n",
      "  \\centering\n",
      "  \\caption{\\textbf{Natural Language Task Accuracies: Base LLMs vs.\\ LLaVA}}\n",
      "  \\label{tab:nl_base_vs_mllm}\n",
      "  \\resizebox{\\linewidth}{!}{%\n",
      "    \\begin{tabular}{l|c|ccccc}\n",
      "      \\toprule\n",
      "      \\textbf{Model} & \\textbf{Instr.} & \\textbf{WSC273} & \\textbf{Winogrande} & \\textbf{ARC‑E} & \\textbf{ARC‑C} & \\textbf{Lambada} \\\\\n",
      "      \\midrule\n",
      "      \\multicolumn{7}{l}{\\textbf{Base LLMs}} \\\\\n",
      "      \\midrule\n",
      "      \\multirow{2}{*}{Pythia (1.4B)} & \\ding{55} & 70.70 & 56.51 & 61.74 & 27.47 & 48.98 \\\\\n",
      "      & \\ding{51} & 59.34 & 50.43 & 48.23 & 26.79 & 33.79 \\\\\n",
      "      \\midrule\n",
      "      \\multirow{2}{*}{LLaMA2 (7B)} & \\ding{51} & 85.35 & 69.53 & 75.63 & 43.17 & 64.35 \\\\\n",
      "      & \\ding{55} & 80.59 & 69.22 & 76.26 & 43.43 & 68.27 \\\\\n",
      "      \\midrule\n",
      "      \\multicolumn{7}{l}{\\textbf{LLaVA MLLMs}} \\\\\n",
      "      \\midrule\n",
      "      \\multirow{2}{*}{Pythia (1.4B)} & \\ding{55} & 67.40 & 56.20 & 60.98 & 27.47 & 40.91 \\\\\n",
      "      & \\ding{51} & 64.84 & 54.06 & 51.94 & 25.68 & 34.80 \\\\\n",
      "      \\midrule\n",
      "      \\multirow{2}{*}{LLaMA2 (7B)} & \\ding{51} & 85.35 & 68.35 & 75.97 & 45.39 & 62.31 \\\\\n",
      "      & \\ding{55} & 87.91 & 70.56 & 77.06 & 44.62 & 68.70 \\\\\n",
      "      \\midrule\n",
      "      \\bottomrule\n",
      "    \\end{tabular}%\n",
      "  }\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "# --- INSERT *AFTER* defining results_dict, models_of_interest, format_value, is_instruction_fine_tuned ---\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# NL tasks & labels\n",
    "nl_cols   = [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]\n",
    "nl_labels = {\n",
    "    \"wsc273\":           \"WSC273\",\n",
    "    \"winogrande\":       \"Winogrande\",\n",
    "    \"arc_easy\":         \"ARC‑E\",\n",
    "    \"arc_challenge\":    \"ARC‑C\",\n",
    "    \"lambada_standard\": \"Lambada\"\n",
    "}\n",
    "\n",
    "# Build baseline LLM entries and LLaVA MLLM entries\n",
    "baseline_entries = []\n",
    "ml_entries       = []\n",
    "for model_key, (baseline_id, label) in models_of_interest.items():\n",
    "    # unify “LLaMA2 Base (7B)” and “LLaMA2 Instruct (7B)” under “LLaMA2 (7B)”\n",
    "    base_label = label.replace(\"LLaVA + \", \"\").replace(\" Base\", \"\")\n",
    "    base_flag  = is_instruction_fine_tuned(label)\n",
    "    baseline_entries.append((base_label, base_flag, baseline_id))\n",
    "\n",
    "    ml_label = label.replace(\"LLaVA + \", \"\").replace(\" Base\", \"\")\n",
    "    ml_flag  = is_instruction_fine_tuned(label)\n",
    "    ml_entries.append((ml_label, ml_flag, model_key))\n",
    "\n",
    "# Deduplicate while preserving order\n",
    "def uniq(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "baseline_entries = uniq(baseline_entries)\n",
    "ml_entries       = uniq(ml_entries)\n",
    "\n",
    "# Group keys (e.g. \"Pythia (1.4B)\", \"LLaMA2 (7B)\")\n",
    "group_keys = []\n",
    "for lbl, _, _ in baseline_entries:\n",
    "    key = lbl.replace(\" Instruct\", \"\")\n",
    "    if key not in group_keys:\n",
    "        group_keys.append(key)\n",
    "\n",
    "# Begin LaTeX table\n",
    "print(r\"\\begin{table*}[h]\")\n",
    "print(r\"  \\centering\")\n",
    "print(r\"  \\caption{\\textbf{Natural Language Task Accuracies: Base LLMs vs.\\ LLaVA}}\")\n",
    "print(r\"  \\label{tab:nl_base_vs_mllm}\")\n",
    "print(r\"  \\resizebox{\\linewidth}{!}{%\")\n",
    "print(r\"    \\begin{tabular}{l|c|\" + \"c\"*len(nl_cols) + r\"}\")\n",
    "print(r\"      \\toprule\")\n",
    "print(r\"      \\textbf{Model} & \\textbf{Instr.} & \" \n",
    "      + \" & \".join([r\"\\textbf{\" + nl_labels[c] + r\"}\" for c in nl_cols]) \n",
    "      + r\" \\\\\")\n",
    "print(r\"      \\midrule\")\n",
    "\n",
    "# Base LLMs block\n",
    "print(r\"      \\multicolumn{\" + str(len(nl_cols)+2) \n",
    "      + r\"}{l}{\\textbf{Base LLMs}} \\\\\")\n",
    "print(r\"      \\midrule\")\n",
    "for key in group_keys:\n",
    "    rows = [(lbl, flag, rid) \n",
    "            for lbl, flag, rid in baseline_entries \n",
    "            if lbl.replace(\" Instruct\",\"\") == key]\n",
    "    for i, (lbl, flag, rid) in enumerate(rows):\n",
    "        tick = r\"\\ding{51}\" if flag else r\"\\ding{55}\"\n",
    "        vals = [ format_value(results_dict[rid].get(c, np.nan)) for c in nl_cols ]\n",
    "        if i == 0:\n",
    "            print(f\"      \\\\multirow{{{len(rows)}}}{{*}}{{{key}}} & {tick} & \" \n",
    "                  + \" & \".join(vals) + r\" \\\\\")\n",
    "        else:\n",
    "            print(f\"      & {tick} & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "    print(r\"      \\midrule\")\n",
    "\n",
    "# LLaVA MLLMs block\n",
    "print(r\"      \\multicolumn{\" + str(len(nl_cols)+2) \n",
    "      + r\"}{l}{\\textbf{LLaVA MLLMs}} \\\\\")\n",
    "print(r\"      \\midrule\")\n",
    "for key in group_keys:\n",
    "    rows = [(lbl, flag, mid) \n",
    "            for lbl, flag, mid in ml_entries \n",
    "            if lbl.replace(\"LLaVA + \",\"\").replace(\" Instruct\",\"\") == key]\n",
    "    for i, (lbl, flag, mid) in enumerate(rows):\n",
    "        tick = r\"\\ding{51}\" if flag else r\"\\ding{55}\"\n",
    "        vals = [ format_value(results_dict[mid].get(c, np.nan)) for c in nl_cols ]\n",
    "        if i == 0:\n",
    "            print(f\"      \\\\multirow{{{len(rows)}}}{{*}}{{{key}}} & {tick} & \" \n",
    "                  + \" & \".join(vals) + r\" \\\\\")\n",
    "        else:\n",
    "            print(f\"      & {tick} & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "    print(r\"      \\midrule\")\n",
    "\n",
    "print(r\"      \\bottomrule\")\n",
    "print(r\"    \\end{tabular}%\")\n",
    "print(r\"  }\")\n",
    "print(r\"\\end{table*}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[h]\n",
      "  \\centering\n",
      "  \\caption{\\textbf{Vision‑Language Task Accuracies: Grouped by Model Family}}\n",
      "  \\label{tab:vl_grouped_accuracies}\n",
      "  \\resizebox{\\linewidth}{!}{%\n",
      "    \\begin{tabular}{l|c|cccc}\n",
      "      \\toprule\n",
      "      \\textbf{Model} & \\textbf{Instr.} & \\textbf{VQA} & \\textbf{TextVQA‑OCR} & \\textbf{TextVQA‑Pure} & \\textbf{GQA} \\\\\n",
      "      \\midrule\n",
      "      \\multirow{2}{*}{Pythia (1.4B)} & \\ding{55} & 66.17 & 38.49 & 35.49 & 46.09 \\\\\n",
      "      & \\ding{51} & 66.46 & 39.12 & 34.35 & 46.88 \\\\\n",
      "      \\midrule\n",
      "      \\multirow{1}{*}{LLaMA2 (7B)} & \\ding{51} & 74.50 & 56.28 & 45.95 & 56.25 \\\\\n",
      "      \\midrule\n",
      "      \\bottomrule\n",
      "    \\end{tabular}%\n",
      "  }\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "# Build MLLM entries (family label, instruct‐flag, lookup key), \n",
    "# but skip the “Base” LLaMA2 entry so only one LLaMA2 (7B) appears\n",
    "ml_entries = []\n",
    "for model_key, (_, label) in models_of_interest.items():\n",
    "    # drop the “Base” version of LLaMA2 to keep only one LLaMA2 (7B)\n",
    "    if \"Base (7B)\" in label:\n",
    "        continue\n",
    "    family    = label.replace(\"LLaVA + \", \"\").replace(\" Instruct\", \"\")\n",
    "    instr_flag = is_instruction_fine_tuned(label)\n",
    "    ml_entries.append((family, instr_flag, model_key))\n",
    "\n",
    "# Extract unique families in order\n",
    "group_keys = []\n",
    "for fam, _, _ in ml_entries:\n",
    "    if fam not in group_keys:\n",
    "        group_keys.append(fam)\n",
    "\n",
    "# Print grouped VL table\n",
    "print(r\"\\begin{table*}[h]\")\n",
    "print(r\"  \\centering\")\n",
    "print(r\"  \\caption{\\textbf{Vision‑Language Task Accuracies: Grouped by Model Family}}\")\n",
    "print(r\"  \\label{tab:vl_grouped_accuracies}\")\n",
    "print(r\"  \\resizebox{\\linewidth}{!}{%\")\n",
    "print(r\"    \\begin{tabular}{l|c|\" + \"c\"*len(vl_cols) + r\"}\")\n",
    "print(r\"      \\toprule\")\n",
    "print(r\"      \\textbf{Model} & \\textbf{Instr.} & \"\n",
    "      + \" & \".join([r\"\\textbf{\" + vl_labels[c] + r\"}\" for c in vl_cols])\n",
    "      + r\" \\\\\")\n",
    "print(r\"      \\midrule\")\n",
    "\n",
    "for fam in group_keys:\n",
    "    rows = [(lbl, flag, key) for lbl, flag, key in ml_entries if lbl == fam]\n",
    "    for i, (lbl, flag, key) in enumerate(rows):\n",
    "        tick = r\"\\ding{51}\" if flag else r\"\\ding{55}\"\n",
    "        vals = [format_value(results_dict[key].get(c, np.nan)) for c in vl_cols]\n",
    "        if i == 0:\n",
    "            print(f\"      \\\\multirow{{{len(rows)}}}{{*}}{{{fam}}} & {tick} & \"\n",
    "                  + \" & \".join(vals) + r\" \\\\\")\n",
    "        else:\n",
    "            print(f\"      & {tick} & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "\n",
    "    print(r\"      \\midrule\")\n",
    "\n",
    "print(r\"      \\bottomrule\")\n",
    "print(r\"    \\end{tabular}%\")\n",
    "print(r\"  }\")\n",
    "print(r\"\\end{table*}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
