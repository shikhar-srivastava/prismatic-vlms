{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model 'stage-final-llava-v15-pythia+160m-soft-old' not found in results\n",
      "{\n",
      "  \"LoRA\": {\n",
      "    \"vqa-v2\": 0.2897,\n",
      "    \"textvqa-ocr\": 0.010156250000000002,\n",
      "    \"textvqa-pure\": 0.017382812500000004,\n",
      "    \"gqa\": 0.1797,\n",
      "    \"refcoco\": 0.0009765625,\n",
      "    \"wsc273\": 0.5457875457875457,\n",
      "    \"winogrande\": 0.4964483030781373,\n",
      "    \"lambada_standard\": 0.1946438967591694,\n",
      "    \"arc_easy\": 0.3888888888888889,\n",
      "    \"arc_challenge\": 0.2167235494880546\n",
      "  },\n",
      "  \"SGM\": {\n",
      "    \"vqa-v2\": 0.2839,\n",
      "    \"textvqa-ocr\": 0.013671875,\n",
      "    \"textvqa-pure\": 0.0271484375,\n",
      "    \"gqa\": 0.1748,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5677655677655677,\n",
      "    \"winogrande\": 0.5074980268350434,\n",
      "    \"lambada_standard\": 0.17698428100135843,\n",
      "    \"arc_easy\": 0.3998316498316498,\n",
      "    \"arc_challenge\": 0.20733788395904437\n",
      "  },\n",
      "  \"Original LLaVA\": {\n",
      "    \"vqa-v2\": 0.3032,\n",
      "    \"textvqa-ocr\": 0.0240234375,\n",
      "    \"textvqa-pure\": 0.038281249999999996,\n",
      "    \"gqa\": 0.2217,\n",
      "    \"refcoco\": 0.00390625,\n",
      "    \"wsc273\": 0.5347985347985348,\n",
      "    \"winogrande\": 0.5193370165745856,\n",
      "    \"lambada_standard\": 0.11313797787696488,\n",
      "    \"arc_easy\": 0.390993265993266,\n",
      "    \"arc_challenge\": 0.20051194539249148\n",
      "  },\n",
      "  \"Language Only LLM\": {\n",
      "    \"vqa-v2\": 0.0,\n",
      "    \"textvqa-ocr\": 0.0,\n",
      "    \"textvqa-pure\": 0.0,\n",
      "    \"gqa\": 0.0,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5677655677655677,\n",
      "    \"winogrande\": 0.4964483030781373,\n",
      "    \"lambada_standard\": 0.23326217737240443,\n",
      "    \"arc_easy\": 0.44234006734006737,\n",
      "    \"arc_challenge\": 0.19965870307167236\n",
      "  },\n",
      "  \"Output Layer Freezing (OLF)\": {\n",
      "    \"vqa-v2\": 0.26940000000000003,\n",
      "    \"textvqa-ocr\": 0.006640625,\n",
      "    \"textvqa-pure\": 0.0361328125,\n",
      "    \"gqa\": 0.1777,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5238095238095238,\n",
      "    \"winogrande\": 0.5343330702446725,\n",
      "    \"lambada_standard\": 0.15447312245294004,\n",
      "    \"arc_easy\": 0.36195286195286197,\n",
      "    \"arc_challenge\": 0.1962457337883959\n",
      "  },\n",
      "  \"Soft Targets + OLF\": {\n",
      "    \"vqa-v2\": 0.2113,\n",
      "    \"textvqa-ocr\": 0.02451171875,\n",
      "    \"textvqa-pure\": 0.025683593749999997,\n",
      "    \"gqa\": 0.1543,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5311355311355311,\n",
      "    \"winogrande\": 0.516179952644041,\n",
      "    \"lambada_standard\": 0.19697263729866096,\n",
      "    \"arc_easy\": 0.4036195286195286,\n",
      "    \"arc_challenge\": 0.23208191126279865\n",
      "  },\n",
      "  \"SGM + OLF\": {\n",
      "    \"vqa-v2\": 0.0097,\n",
      "    \"textvqa-ocr\": 0.011816406250000001,\n",
      "    \"textvqa-pure\": 0.0021484375,\n",
      "    \"gqa\": 0.0078000000000000005,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5604395604395604,\n",
      "    \"winogrande\": 0.5059194948697711,\n",
      "    \"lambada_standard\": 0.21036289540073744,\n",
      "    \"arc_easy\": 0.41624579124579125,\n",
      "    \"arc_challenge\": 0.2022184300341297\n",
      "  },\n",
      "  \"IA3\": {\n",
      "    \"vqa-v2\": 0.0,\n",
      "    \"textvqa-ocr\": 0.0,\n",
      "    \"textvqa-pure\": 0.0,\n",
      "    \"gqa\": 0.0,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.575091575091575,\n",
      "    \"winogrande\": 0.4964483030781373,\n",
      "    \"lambada_standard\": 0.18455268775470599,\n",
      "    \"arc_easy\": 0.44107744107744107,\n",
      "    \"arc_challenge\": 0.20136518771331058\n",
      "  },\n",
      "  \"LoRA (1/4 Full Rank)\": {\n",
      "    \"vqa-v2\": 0.2464,\n",
      "    \"textvqa-ocr\": 0.00927734375,\n",
      "    \"textvqa-pure\": 0.014062500000000004,\n",
      "    \"gqa\": 0.15039999999999998,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5311355311355311,\n",
      "    \"winogrande\": 0.5011838989739542,\n",
      "    \"lambada_standard\": 0.08111779545895595,\n",
      "    \"arc_easy\": 0.3653198653198653,\n",
      "    \"arc_challenge\": 0.20392491467576793\n",
      "  },\n",
      "  \"LoRA (1/4 Full Rank, Higher Alpha)\": {\n",
      "    \"vqa-v2\": 0.0646,\n",
      "    \"textvqa-ocr\": 0.0068359375,\n",
      "    \"textvqa-pure\": 0.00546875,\n",
      "    \"gqa\": 0.024399999999999998,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5384615384615384,\n",
      "    \"winogrande\": 0.4972375690607735,\n",
      "    \"lambada_standard\": 0.18707549000582185,\n",
      "    \"arc_easy\": 0.3813131313131313,\n",
      "    \"arc_challenge\": 0.2090443686006826\n",
      "  },\n",
      "  \"LoRA (1/2 Full Rank, Higher Alpha)\": {\n",
      "    \"vqa-v2\": 0.2872,\n",
      "    \"textvqa-ocr\": 0.010546875,\n",
      "    \"textvqa-pure\": 0.02666015625,\n",
      "    \"gqa\": 0.1973,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": 0.5860805860805861,\n",
      "    \"winogrande\": 0.5043409629044988,\n",
      "    \"lambada_standard\": 0.10149427517950708,\n",
      "    \"arc_easy\": 0.3872053872053872,\n",
      "    \"arc_challenge\": 0.18686006825938567\n",
      "  },\n",
      "  \"LoRA (1/2 Full Rank)\": {\n",
      "    \"vqa-v2\": 0.0013,\n",
      "    \"textvqa-ocr\": 0.001953125,\n",
      "    \"textvqa-pure\": 0.0009765625,\n",
      "    \"gqa\": 0.0,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": NaN,\n",
      "    \"winogrande\": NaN,\n",
      "    \"lambada_standard\": NaN,\n",
      "    \"arc_easy\": NaN,\n",
      "    \"arc_challenge\": NaN\n",
      "  },\n",
      "  \"LoRA (1/2 Full Rank, RSLoRA)\": {\n",
      "    \"vqa-v2\": 0.2897,\n",
      "    \"textvqa-ocr\": 0.010156250000000002,\n",
      "    \"textvqa-pure\": 0.017382812500000004,\n",
      "    \"gqa\": 0.1797,\n",
      "    \"refcoco\": 0.0009765625,\n",
      "    \"wsc273\": 0.5457875457875457,\n",
      "    \"winogrande\": 0.4964483030781373,\n",
      "    \"lambada_standard\": 0.1946438967591694,\n",
      "    \"arc_easy\": 0.3888888888888889,\n",
      "    \"arc_challenge\": 0.2167235494880546\n",
      "  },\n",
      "  \"LoRA (1/2 Full Rank, RSLoRA, KQV Target)\": {\n",
      "    \"vqa-v2\": 0.0,\n",
      "    \"textvqa-ocr\": 0.0,\n",
      "    \"textvqa-pure\": 0.0,\n",
      "    \"gqa\": 0.0,\n",
      "    \"refcoco\": 0.0,\n",
      "    \"wsc273\": NaN,\n",
      "    \"winogrande\": NaN,\n",
      "    \"lambada_standard\": NaN,\n",
      "    \"arc_easy\": NaN,\n",
      "    \"arc_challenge\": NaN\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the datasets of interest\n",
    "datasets = [\n",
    "    'vqa-v2', 'textvqa-ocr', 'textvqa-pure', 'gqa', 'refcoco',\n",
    "    'wsc273', 'winogrande', 'lambada_standard', 'arc_easy', 'arc_challenge'\n",
    "]\n",
    "\n",
    "# Define the result dictionary\n",
    "result = {}\n",
    "\n",
    "# Path to the JSON results file\n",
    "results_file = 'results_nlp.json'\n",
    "\n",
    "# Read the JSON results file\n",
    "if os.path.isfile(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        result = json.load(f)\n",
    "else:\n",
    "    print(f\"Error: File {results_file} not found.\")\n",
    "\n",
    "# Mapping from different methods to model names\n",
    "model_mappings = {\n",
    "    'soft': 'stage-final-llava-v15-pythia+160m-soft-old',\n",
    "    'lora': 'stage-final-llava-v15-pythia+160m-lora',\n",
    "    'sgm': 'stage-final-llava-v15-pythia+160m-sgm',\n",
    "    'original': 'stage-final-llava-v15-pythia+160m',\n",
    "    'vicuna': 'reproduction-align-pythia+160m',\n",
    "    'olf': 'stage-final-llava-v15-pythia+160m-olf',\n",
    "    'softolf': 'stage-final-llava-v15-pythia+160m-softolf',\n",
    "    'sgmolf': 'stage-final-llava-v15-pythia+160m-sgmolf',\n",
    "    'ia3': 'stage-final-llava-v15-pythia+160m-ia3',\n",
    "    'lora-quarterfullrank': 'stage-final-llava-v15-pythia+160m-lora-use_rslora-rank_by_factor_4',\n",
    "    'lora-quarterfullrank-higheralpha': 'stage-final-llava-v15-pythia+160m-lora-use_rslora-rank_by_factor_4-higheralpha-16',\n",
    "    'lora-halffullrank-higheralpha': 'stage-final-llava-v15-pythia+160m-lora-use_rslora-rank_by_factor_2-higheralpha-32',\n",
    "    'lora-halffullrank': 'stage-final-llava-v15-pythia+160m-lora-rank_by_factor_2',\n",
    "    'lora-halffullrank-rslora': 'stage-final-llava-v15-pythia+160m-lora-use_rslora-rank_by_factor_2',\n",
    "    'lora-halffullrank-rslora-kqv': 'stage-final-llava-v15-pythia+160m-lora-use_rslora-higher_rank-target_modules_kqv'\n",
    "}\n",
    "\n",
    "# Label name mappings for the main methods and variants of LoRA\n",
    "name_mapping = {\n",
    "    'soft': 'Soft Targets',\n",
    "    'softolf': 'Soft Targets + OLF',\n",
    "    'sgmolf': 'SGM + OLF',\n",
    "    'sgm': 'SGM',\n",
    "    'lora': 'LoRA',\n",
    "    'ia3': 'IA3',\n",
    "    'original': 'Original LLaVA',\n",
    "    'vicuna': 'Language Only LLM',\n",
    "    'olf': 'Output Layer Freezing (OLF)',\n",
    "    'lora-old': 'LoRA (Rank 16, Alpha 8)',\n",
    "    'lora-quarterfullrank': 'LoRA (1/4 Full Rank)',\n",
    "    'lora-quarterfullrank-higheralpha': 'LoRA (1/4 Full Rank, Higher Alpha)',\n",
    "    'lora-halffullrank-higheralpha': 'LoRA (1/2 Full Rank, Higher Alpha)',\n",
    "    'lora-halffullrank': 'LoRA (1/2 Full Rank)',\n",
    "    'lora-halffullrank-rslora': 'LoRA (1/2 Full Rank, RSLoRA)',\n",
    "    'lora-halffullrank-rslora-kqv': 'LoRA (1/2 Full Rank, RSLoRA, KQV Target)',\n",
    "}\n",
    "\n",
    "# Initialize model mappings\n",
    "model_results = {model: [] for model in model_mappings.keys()}\n",
    "\n",
    "# Populate the mappings based on the given mappings\n",
    "for method, model_name in model_mappings.items():\n",
    "    if model_name in result:\n",
    "        metrics = result[model_name]\n",
    "        accuracies = list(metrics.values())\n",
    "        avg_accuracy = np.nanmean(accuracies)\n",
    "        model_results[method].append((model_name, avg_accuracy))\n",
    "    else:\n",
    "        print(f\"Warning: Model '{model_name}' not found in results\")\n",
    "\n",
    "# Identify the highest accuracy model for each mapping\n",
    "highest_accuracy_models = {}\n",
    "for method, mappings in model_results.items():\n",
    "    if mappings:\n",
    "        highest_accuracy_models[method] = max(mappings, key=lambda x: x[1])\n",
    "\n",
    "# Save the highest accuracy models to a file\n",
    "output_file = 'highest_accuracy_models.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(highest_accuracy_models, f, indent=2)\n",
    "\n",
    "# Prepare data for radar chart\n",
    "methods = [name_mapping[model] for model in model_mappings.keys() if model in highest_accuracy_models]\n",
    "results_dict = {}\n",
    "\n",
    "for model in model_mappings.keys():\n",
    "    if model in highest_accuracy_models:\n",
    "        model_name, _ = highest_accuracy_models[model]\n",
    "        metrics = result[model_name]\n",
    "        accuracies = {dataset: metrics.get(dataset, np.nan) for dataset in datasets}\n",
    "        results_dict[name_mapping[model]] = accuracies\n",
    "\n",
    "# Output the results dictionary\n",
    "print(json.dumps(results_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "original_llava_acc = results_dict[\"Original LLaVA\"]\n",
    "language_only_llm_acc = results_dict.get(\"Language Only LLM\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model in [\"Original LLaVA\", \"Language Only LLM\"]:\n",
    "        continue\n",
    "    deltas = {dataset: acc - original_llava_acc[dataset] for dataset, acc in accuracies.items()}\n",
    "    avg_delta_vl = sum(deltas[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    avg_delta_nlu = sum(deltas[dataset] for dataset in accuracies if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]) / 5\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\", \"lambada_standard\"]) / 5\n",
    "    table_data.append((model, accuracies, deltas, avg_delta_vl, avg_acc_vl, avg_delta_nlu, avg_acc_nlu))\n",
    "\n",
    "# Sort the data so models with smallest average delta VL are first\n",
    "table_data = sorted(table_data, key=lambda x: x[3])\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Generate LaTeX table for Vision-Language part\n",
    "latex_code_vl = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Vision-Language (VL) Tasks}\n",
    "  \\\\label{tab:vl_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\multicolumn{2}{c}{\\\\textbf{VL Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & $\\\\Delta$ & Acc \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code_vl += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & - & {avg_acc_vl} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    avg_acc_vl=format_value((original_llava_acc[\"vqa-v2\"] + original_llava_acc[\"textvqa-ocr\"] + original_llava_acc[\"textvqa-pure\"] + original_llava_acc[\"gqa\"]) / 4)\n",
    ")\n",
    "\n",
    "latex_code_vl += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & - & {avg_acc_vl} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    avg_acc_vl=format_value((language_only_llm_acc.get(\"vqa-v2\", 0) + language_only_llm_acc.get(\"textvqa-ocr\", 0) + language_only_llm_acc.get(\"textvqa-pure\", 0) + language_only_llm_acc.get(\"gqa\", 0)) / 4)\n",
    ")\n",
    "\n",
    "latex_code_vl += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, deltas, avg_delta_vl, avg_acc_vl, _, _ in table_data:\n",
    "    latex_code_vl += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {delta_vl} & {acc_vl} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        delta_vl=format_value(avg_delta_vl),\n",
    "        acc_vl=format_value(avg_acc_vl)\n",
    "    )\n",
    "\n",
    "latex_code_vl += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code_vl)\n",
    "\n",
    "# Generate LaTeX table for NLU/NLG part\n",
    "latex_code_nlu = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} NLU/NLG Tasks}\n",
    "  \\\\label{tab:nlu_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|ccccc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{5}{c|}{\\\\textbf{NLU/NLG}} & \\\\multicolumn{2}{c}{\\\\textbf{NLU Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{WSC273} & \\\\textbf{Winogrande} & \\\\textbf{ARC Easy} & \\\\textbf{ARC Challenge} & \\\\textbf{Lambada} & $\\\\Delta$ & Acc \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code_nlu += \"Original LLaVA & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} & - & {avg_acc_nlu} \\\\\\\\\\n\".format(\n",
    "    wsc273=format_value(original_llava_acc[\"wsc273\"]),\n",
    "    winogrande=format_value(original_llava_acc[\"winogrande\"]),\n",
    "    arc_easy=format_value(original_llava_acc[\"arc_easy\"]),\n",
    "    arc_challenge=format_value(original_llava_acc[\"arc_challenge\"]),\n",
    "    lambada=format_value(original_llava_acc[\"lambada_standard\"]),\n",
    "    avg_acc_nlu=format_value((original_llava_acc[\"wsc273\"] + original_llava_acc[\"winogrande\"] + original_llava_acc[\"arc_easy\"] + original_llava_acc[\"arc_challenge\"] + original_llava_acc[\"lambada_standard\"]) / 5)\n",
    ")\n",
    "\n",
    "latex_code_nlu += \"Language Only LLM & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} & - & {avg_acc_nlu} \\\\\\\\\\n\".format(\n",
    "    wsc273=format_value(language_only_llm_acc.get(\"wsc273\", np.nan)),\n",
    "    winogrande=format_value(language_only_llm_acc.get(\"winogrande\", np.nan)),\n",
    "    arc_easy=format_value(language_only_llm_acc.get(\"arc_easy\", np.nan)),\n",
    "    arc_challenge=format_value(language_only_llm_acc.get(\"arc_challenge\", np.nan)),\n",
    "    lambada=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan)),\n",
    "    avg_acc_nlu=format_value((language_only_llm_acc.get(\"wsc273\", 0) + language_only_llm_acc.get(\"winogrande\", 0) + language_only_llm_acc.get(\"arc_easy\", 0) + language_only_llm_acc.get(\"arc_challenge\", 0) + language_only_llm_acc.get(\"lambada_standard\", 0)) / 5)\n",
    ")\n",
    "\n",
    "latex_code_nlu += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, deltas, _, _, avg_delta_nlu, avg_acc_nlu in table_data:\n",
    "    latex_code_nlu += \"{model} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} & {delta_nlu} & {acc_nlu} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        wsc273=format_value(accuracies[\"wsc273\"]),\n",
    "        winogrande=format_value(accuracies[\"winogrande\"]),\n",
    "        arc_easy=format_value(accuracies[\"arc_easy\"]),\n",
    "        arc_challenge=format_value(accuracies[\"arc_challenge\"]),\n",
    "        lambada=format_value(accuracies[\"lambada_standard\"]),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        acc_nlu=format_value(avg_acc_nlu)\n",
    "    )\n",
    "\n",
    "latex_code_nlu += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code_nlu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance:} LoRA Variants}\n",
      "  \\label{tab:lora_variants_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccc|c|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{4}{c|}{\\textbf{Vision-Language (VL)}} & \\textbf{VL Avg.} & \\multicolumn{2}{c|}{\\textbf{NLU Avg.}} & \\multicolumn{2}{c}{\\textbf{NLG Avg.}} \\\\\n",
      "     & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 30.3 & 2.4 & 3.8 & 22.2 & 14.7 & -1.5 & 41.1 & -12.0 & 11.3 \\\\\n",
      "Language Only LLM & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & - & 42.7 & - & 23.3 \\\\\n",
      "\\midrule\n",
      "LoRA (1/2 Full Rank, Higher Alpha) & 28.7 & 1.1 & 2.7 & 19.7 & 13.0 & -1.0 & 41.6 & -13.2 & 10.1 \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA) & 29.0 & 1.0 & 1.7 & 18.0 & 12.4 & -1.5 & 41.2 & -3.9 & 19.5 \\\\\n",
      "LoRA (1/4 Full Rank) & 24.6 & 0.9 & 1.4 & 15.0 & 10.5 & -2.6 & 40.0 & -15.2 & 8.1 \\\\\n",
      "LoRA (1/4 Full Rank, Higher Alpha) & 6.5 & 0.7 & 0.5 & 2.4 & 2.5 & -2.0 & 40.7 & -4.6 & 18.7 \\\\\n",
      "LoRA (1/2 Full Rank) & 0.1 & 0.2 & 0.1 & 0.0 & 0.1 & - & - & - & - \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA, KQV Target) & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & - & - & - & - \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance:} Other Methods}\n",
      "  \\label{tab:other_methods_acc}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccc|c|cc|cc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\multicolumn{4}{c|}{\\textbf{Vision-Language (VL)}} & \\textbf{VL Avg.} & \\multicolumn{2}{c|}{\\textbf{NLU Avg.}} & \\multicolumn{2}{c}{\\textbf{NLG Avg.}} \\\\\n",
      "     & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ & $\\Delta \\uparrow$ & Acc $\\uparrow$ \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 30.3 & 2.4 & 3.8 & 22.2 & 14.7 & -1.5 & 41.1 & -12.0 & 11.3 \\\\\n",
      "Language Only LLM & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & - & 42.7 & - & 23.3 \\\\\n",
      "\\midrule\n",
      "SGM & 28.4 & 1.4 & 2.7 & 17.5 & 12.5 & -0.6 & 42.1 & -5.6 & 17.7 \\\\\n",
      "LoRA & 29.0 & 1.0 & 1.7 & 18.0 & 12.4 & -1.5 & 41.2 & -3.9 & 19.5 \\\\\n",
      "Output Layer Freezing (OLF) & 26.9 & 0.7 & 3.6 & 17.8 & 12.2 & -2.2 & 40.4 & -7.9 & 15.4 \\\\\n",
      "Soft Targets + OLF & 21.1 & 2.5 & 2.6 & 15.4 & 10.4 & -0.6 & 42.1 & -3.6 & 19.7 \\\\\n",
      "SGM + OLF & 1.0 & 1.2 & 0.2 & 0.8 & 0.8 & -0.5 & 42.1 & -2.3 & 21.0 \\\\\n",
      "IA3 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.2 & 42.8 & -4.9 & 18.5 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "original_llava_acc = results_dict[\"Original LLaVA\"]\n",
    "language_only_llm_acc = results_dict.get(\"Language Only LLM\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model in [\"Original LLaVA\", \"Language Only LLM\"]:\n",
    "        continue\n",
    "    avg_acc_vl = sum(accuracies[dataset] for dataset in accuracies if dataset in [\"vqa-v2\", \"textvqa-ocr\", \"textvqa-pure\", \"gqa\"]) / 4\n",
    "    nlu_deltas = {dataset: accuracies[dataset] - language_only_llm_acc.get(dataset, 0) for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]}\n",
    "    avg_delta_nlu = sum(nlu_deltas[dataset] for dataset in nlu_deltas) / 4\n",
    "    avg_acc_nlu = sum(accuracies[dataset] for dataset in nlu_deltas) / 4\n",
    "    delta_nlg = accuracies[\"lambada_standard\"] - language_only_llm_acc.get(\"lambada_standard\", 0)\n",
    "    avg_acc_nlg = accuracies[\"lambada_standard\"]\n",
    "    table_data.append((model, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg))\n",
    "\n",
    "# Sort the data by Avg. VL Accuracy and highest NLG Delta\n",
    "table_data = sorted(table_data, key=lambda x: (x[2], -x[5]), reverse=True)\n",
    "\n",
    "# Separate the data into two groups\n",
    "lora_variants = [\n",
    "    'LoRA (1/4 Full Rank)', 'LoRA (1/4 Full Rank, Higher Alpha)', \n",
    "    'LoRA (1/2 Full Rank)', 'LoRA (1/2 Full Rank, Higher Alpha)', \n",
    "    'LoRA (1/2 Full Rank, RSLoRA)', 'LoRA (1/2 Full Rank, RSLoRA, KQV Target)'\n",
    "]\n",
    "\n",
    "lora_table_data = [item for item in table_data if item[0] in lora_variants]\n",
    "other_table_data = [item for item in table_data if item[0] not in lora_variants]\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Generate LaTeX table for LoRA variants\n",
    "latex_code_lora = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} LoRA Variants}\n",
    "  \\\\label{tab:lora_variants_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|c|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\textbf{VL Avg.} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NLG Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "original_llava_deltas = {dataset: original_llava_acc[dataset] - language_only_llm_acc.get(dataset, 0) for dataset in [\"wsc273\", \"winogrande\", \"arc_easy\", \"arc_challenge\"]}\n",
    "original_llava_avg_delta_nlu = sum(original_llava_deltas[dataset] for dataset in original_llava_deltas) / 4\n",
    "original_llava_delta_nlg = original_llava_acc[\"lambada_standard\"] - language_only_llm_acc.get(\"lambada_standard\", 0)\n",
    "\n",
    "latex_code_lora += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & {delta_nlu} & {avg_acc_nlu} & {delta_nlg} & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    avg_acc_vl=format_value((original_llava_acc[\"vqa-v2\"] + original_llava_acc[\"textvqa-ocr\"] + original_llava_acc[\"textvqa-pure\"] + original_llava_acc[\"gqa\"]) / 4),\n",
    "    delta_nlu=format_value(original_llava_avg_delta_nlu),\n",
    "    avg_acc_nlu=format_value((original_llava_acc[\"wsc273\"] + original_llava_acc[\"winogrande\"] + original_llava_acc[\"arc_easy\"] + original_llava_acc[\"arc_challenge\"]) / 4),\n",
    "    delta_nlg=format_value(original_llava_delta_nlg),\n",
    "    avg_acc_nlg=format_value(original_llava_acc[\"lambada_standard\"])\n",
    ")\n",
    "\n",
    "latex_code_lora += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & - & {avg_acc_nlu} & - & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    avg_acc_vl=format_value((language_only_llm_acc.get(\"vqa-v2\", 0) + language_only_llm_acc.get(\"textvqa-ocr\", 0) + language_only_llm_acc.get(\"textvqa-pure\", 0) + language_only_llm_acc.get(\"gqa\", 0)) / 4),\n",
    "    avg_acc_nlu=format_value((language_only_llm_acc.get(\"wsc273\", 0) + language_only_llm_acc.get(\"winogrande\", 0) + language_only_llm_acc.get(\"arc_easy\", 0) + language_only_llm_acc.get(\"arc_challenge\", 0)) / 4),\n",
    "    avg_acc_nlg=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan))\n",
    ")\n",
    "\n",
    "latex_code_lora += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg in lora_table_data:\n",
    "    latex_code_lora += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {acc_vl} & {delta_nlu} & {acc_nlu} & {delta_nlg} & {acc_nlg} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        acc_vl=format_value(avg_acc_vl),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        acc_nlu=format_value(avg_acc_nlu),\n",
    "        delta_nlg=format_value(delta_nlg),\n",
    "        acc_nlg=format_value(avg_acc_nlg)\n",
    "    )\n",
    "\n",
    "latex_code_lora += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "# Generate LaTeX table for other methods\n",
    "latex_code_other = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Other Methods}\n",
    "  \\\\label{tab:other_methods_acc}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccc|c|cc|cc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\multicolumn{4}{c|}{\\\\textbf{Vision-Language (VL)}} & \\\\textbf{VL Avg.} & \\\\multicolumn{2}{c|}{\\\\textbf{NLU Avg.}} & \\\\multicolumn{2}{c}{\\\\textbf{NLG Avg.}} \\\\\\\\\n",
    "     & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ & $\\\\Delta \\\\uparrow$ & Acc $\\\\uparrow$ \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code_other += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & {delta_nlu} & {avg_acc_nlu} & {delta_nlg} & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    avg_acc_vl=format_value((original_llava_acc[\"vqa-v2\"] + original_llava_acc[\"textvqa-ocr\"] + original_llava_acc[\"textvqa-pure\"] + original_llava_acc[\"gqa\"]) / 4),\n",
    "    delta_nlu=format_value(original_llava_avg_delta_nlu),\n",
    "    avg_acc_nlu=format_value((original_llava_acc[\"wsc273\"] + original_llava_acc[\"winogrande\"] + original_llava_acc[\"arc_easy\"] + original_llava_acc[\"arc_challenge\"]) / 4),\n",
    "    delta_nlg=format_value(original_llava_delta_nlg),\n",
    "    avg_acc_nlg=format_value(original_llava_acc[\"lambada_standard\"])\n",
    ")\n",
    "\n",
    "latex_code_other += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {avg_acc_vl} & - & {avg_acc_nlu} & - & {avg_acc_nlg} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    avg_acc_vl=format_value((language_only_llm_acc.get(\"vqa-v2\", 0) + language_only_llm_acc.get(\"textvqa-ocr\", 0) + language_only_llm_acc.get(\"textvqa-pure\", 0) + language_only_llm_acc.get(\"gqa\", 0)) / 4),\n",
    "    avg_acc_nlu=format_value((language_only_llm_acc.get(\"wsc273\", 0) + language_only_llm_acc.get(\"winogrande\", 0) + language_only_llm_acc.get(\"arc_easy\", 0) + language_only_llm_acc.get(\"arc_challenge\", 0)) / 4),\n",
    "    avg_acc_nlg=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan))\n",
    ")\n",
    "\n",
    "latex_code_other += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies, avg_acc_vl, avg_delta_nlu, avg_acc_nlu, delta_nlg, avg_acc_nlg in other_table_data:\n",
    "    latex_code_other += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {acc_vl} & {delta_nlu} & {acc_nlu} & {delta_nlg} & {acc_nlg} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        acc_vl=format_value(avg_acc_vl),\n",
    "        delta_nlu=format_value(avg_delta_nlu),\n",
    "        acc_nlu=format_value(avg_acc_nlu),\n",
    "        delta_nlg=format_value(delta_nlg),\n",
    "        acc_nlg=format_value(avg_acc_nlg)\n",
    "    )\n",
    "\n",
    "latex_code_other += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code_lora)\n",
    "print(latex_code_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "  \\caption{\\textbf{LLaVA Model Performance:} Per Dataset}\n",
      "  \\label{tab:per_dataset}\n",
      "  \\centering\n",
      "  \\resizebox{\\linewidth}{!}{\n",
      "    \\begin{tabular}{l|cccccccccc}\n",
      "     \\toprule\n",
      "     \\textbf{Model} & \\textbf{VQAv2} & \\textbf{TextVQA OCR} & \\textbf{TextVQA Pure} & \\textbf{GQA} & \\textbf{WSC273} & \\textbf{Winogrande} & \\textbf{ARC Easy} & \\textbf{ARC Challenge} & \\textbf{Lambada} \\\\\n",
      "     \\midrule\n",
      "Original LLaVA & 30.3 & 2.4 & 3.8 & 22.2 & 53.5 & 51.9 & 39.1 & 20.1 & 11.3 \\\\\n",
      "Language Only LLM & 0.0 & 0.0 & 0.0 & 0.0 & 56.8 & 49.6 & 44.2 & 20.0 & 23.3 \\\\\n",
      "\\midrule\n",
      "IA3 & 0.0 & 0.0 & 0.0 & 0.0 & 57.5 & 49.6 & 44.1 & 20.1 & 18.5 \\\\\n",
      "LoRA & 29.0 & 1.0 & 1.7 & 18.0 & 54.6 & 49.6 & 38.9 & 21.7 & 19.5 \\\\\n",
      "LoRA (1/2 Full Rank) & 0.1 & 0.2 & 0.1 & 0.0 & - & - & - & - & - \\\\\n",
      "LoRA (1/2 Full Rank, Higher Alpha) & 28.7 & 1.1 & 2.7 & 19.7 & 58.6 & 50.4 & 38.7 & 18.7 & 10.1 \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA) & 29.0 & 1.0 & 1.7 & 18.0 & 54.6 & 49.6 & 38.9 & 21.7 & 19.5 \\\\\n",
      "LoRA (1/2 Full Rank, RSLoRA, KQV Target) & 0.0 & 0.0 & 0.0 & 0.0 & - & - & - & - & - \\\\\n",
      "LoRA (1/4 Full Rank) & 24.6 & 0.9 & 1.4 & 15.0 & 53.1 & 50.1 & 36.5 & 20.4 & 8.1 \\\\\n",
      "LoRA (1/4 Full Rank, Higher Alpha) & 6.5 & 0.7 & 0.5 & 2.4 & 53.8 & 49.7 & 38.1 & 20.9 & 18.7 \\\\\n",
      "Output Layer Freezing (OLF) & 26.9 & 0.7 & 3.6 & 17.8 & 52.4 & 53.4 & 36.2 & 19.6 & 15.4 \\\\\n",
      "SGM & 28.4 & 1.4 & 2.7 & 17.5 & 56.8 & 50.7 & 40.0 & 20.7 & 17.7 \\\\\n",
      "SGM + OLF & 1.0 & 1.2 & 0.2 & 0.8 & 56.0 & 50.6 & 41.6 & 20.2 & 21.0 \\\\\n",
      "Soft Targets + OLF & 21.1 & 2.5 & 2.6 & 15.4 & 53.1 & 51.6 & 40.4 & 23.2 & 19.7 \\\\\n",
      "\n",
      "     \\bottomrule\n",
      "    \\end{tabular}\n",
      "  }\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate delta values\n",
    "original_llava_acc = results_dict[\"Original LLaVA\"]\n",
    "language_only_llm_acc = results_dict.get(\"Language Only LLM\", {})\n",
    "\n",
    "table_data = []\n",
    "\n",
    "for model, accuracies in results_dict.items():\n",
    "    if model not in [\"Original LLaVA\", \"Language Only LLM\"]:\n",
    "        table_data.append((model, accuracies))\n",
    "\n",
    "# Sort the data by model name\n",
    "table_data = sorted(table_data, key=lambda x: x[0])\n",
    "\n",
    "# Function to format values or return \"-\"\n",
    "def format_value(value):\n",
    "    return \"{:.1f}\".format(value * 100) if not np.isnan(value) else \"-\"\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_code = \"\"\"\n",
    "\\\\begin{table*}[h]\n",
    "  \\\\caption{\\\\textbf{LLaVA Model Performance:} Per Dataset}\n",
    "  \\\\label{tab:per_dataset}\n",
    "  \\\\centering\n",
    "  \\\\resizebox{\\\\linewidth}{!}{\n",
    "    \\\\begin{tabular}{l|cccccccccc}\n",
    "     \\\\toprule\n",
    "     \\\\textbf{Model} & \\\\textbf{VQAv2} & \\\\textbf{TextVQA OCR} & \\\\textbf{TextVQA Pure} & \\\\textbf{GQA} & \\\\textbf{WSC273} & \\\\textbf{Winogrande} & \\\\textbf{ARC Easy} & \\\\textbf{ARC Challenge} & \\\\textbf{Lambada} \\\\\\\\\n",
    "     \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "latex_code += \"Original LLaVA & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(original_llava_acc[\"vqa-v2\"]),\n",
    "    textvqa_ocr=format_value(original_llava_acc[\"textvqa-ocr\"]),\n",
    "    textvqa_pure=format_value(original_llava_acc[\"textvqa-pure\"]),\n",
    "    gqa=format_value(original_llava_acc[\"gqa\"]),\n",
    "    wsc273=format_value(original_llava_acc[\"wsc273\"]),\n",
    "    winogrande=format_value(original_llava_acc[\"winogrande\"]),\n",
    "    arc_easy=format_value(original_llava_acc[\"arc_easy\"]),\n",
    "    arc_challenge=format_value(original_llava_acc[\"arc_challenge\"]),\n",
    "    lambada=format_value(original_llava_acc[\"lambada_standard\"])\n",
    ")\n",
    "\n",
    "latex_code += \"Language Only LLM & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} \\\\\\\\\\n\".format(\n",
    "    vqa_v2=format_value(language_only_llm_acc.get(\"vqa-v2\", np.nan)),\n",
    "    textvqa_ocr=format_value(language_only_llm_acc.get(\"textvqa-ocr\", np.nan)),\n",
    "    textvqa_pure=format_value(language_only_llm_acc.get(\"textvqa-pure\", np.nan)),\n",
    "    gqa=format_value(language_only_llm_acc.get(\"gqa\", np.nan)),\n",
    "    wsc273=format_value(language_only_llm_acc.get(\"wsc273\", np.nan)),\n",
    "    winogrande=format_value(language_only_llm_acc.get(\"winogrande\", np.nan)),\n",
    "    arc_easy=format_value(language_only_llm_acc.get(\"arc_easy\", np.nan)),\n",
    "    arc_challenge=format_value(language_only_llm_acc.get(\"arc_challenge\", np.nan)),\n",
    "    lambada=format_value(language_only_llm_acc.get(\"lambada_standard\", np.nan))\n",
    ")\n",
    "\n",
    "latex_code += \"\\\\midrule\\n\"\n",
    "\n",
    "for model, accuracies in table_data:\n",
    "    latex_code += \"{model} & {vqa_v2} & {textvqa_ocr} & {textvqa_pure} & {gqa} & {wsc273} & {winogrande} & {arc_easy} & {arc_challenge} & {lambada} \\\\\\\\\\n\".format(\n",
    "        model=model,\n",
    "        vqa_v2=format_value(accuracies[\"vqa-v2\"]),\n",
    "        textvqa_ocr=format_value(accuracies[\"textvqa-ocr\"]),\n",
    "        textvqa_pure=format_value(accuracies[\"textvqa-pure\"]),\n",
    "        gqa=format_value(accuracies[\"gqa\"]),\n",
    "        wsc273=format_value(accuracies[\"wsc273\"]),\n",
    "        winogrande=format_value(accuracies[\"winogrande\"]),\n",
    "        arc_easy=format_value(accuracies[\"arc_easy\"]),\n",
    "        arc_challenge=format_value(accuracies[\"arc_challenge\"]),\n",
    "        lambada=format_value(accuracies[\"lambada_standard\"])\n",
    "    )\n",
    "\n",
    "latex_code += \"\"\"\n",
    "     \\\\bottomrule\n",
    "    \\\\end{tabular}\n",
    "  }\n",
    "\\\\end{table*}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPTNeoXForCausalLM from transformers\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoConfig\n",
    "# Load pythia-160m model from Hugging Face\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/pythia-1.4b-deduped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
